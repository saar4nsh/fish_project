# -*- coding: utf-8 -*-
"""GeneDetectionSukrit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Jo8_KOd8BjL-AJmE1PJ1p9DG3cDpMHzD

## Imports
"""

import os
import numpy as np
from PIL import Image
import torch
import torchvision
import torchvision.transforms as T
from torch.utils.data import Dataset, DataLoader
from torchvision.models.detection import maskrcnn_resnet50_fpn
from torchvision.transforms import functional as F
from torchvision.models.detection.anchor_utils import AnchorGenerator
from scipy.ndimage import label as cc_label
from collections import defaultdict
import time

from tqdm import tqdm
import cv2
import random

# from google.colab import drive
# drive.mount('/content/data/')

"""## Utility"""

# DEVICE = torch.device("cpu")

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# DEVICE = torch.cuda.device(0)
NUM_CLASSES = 5
MIN_PIXELS = 4
MIN_AREA = 20
BATCH_SIZE = 4 
MAX_PATCHES = 20
NUM_EPOCHS = 20

CLASS_COLORS = {
    'yellow': ([0, 128, 0], 1),
    'green': ([0, 128, 0], 2),
    'aqua': ([0, 0, 128], 3),
    'fusion': ([128, 0, 128], 4),
    'red': ([128, 0, 0], 0)
}

LABEL_TO_CLASS = {v: k for k, (_, v) in CLASS_COLORS.items()}
COLOR_TO_LABEL = {tuple(v): k for k, (v, k) in CLASS_COLORS.items()}

# Extracts individual binary masks and class labels for each gene instance from a semantic segmentation mask
def extract_instances_from_mask(mask_image, min_pixels=2):
    mask = np.array(mask_image.convert("RGB"))

    # Updated color map from user
    CLASS_COLORS = {
        'red': (0, 128, 0),
        'green': (128, 128, 0),
        'fusion': (128, 0, 128),
        'aqua': (0, 0, 128),
        'red': (128, 0, 0)
    }
    COLOR_TO_LABEL = {v: i+1 for i, v in enumerate(CLASS_COLORS.values())}

    binary_masks = []
    labels = []

    for rgb, label in COLOR_TO_LABEL.items():
        binary_mask = np.all(mask == rgb, axis=-1).astype(np.uint8)
        labeled_mask, num_instances = cc_label(binary_mask)

        for inst_id in range(1, num_instances + 1):
            instance = (labeled_mask == inst_id).astype(np.uint8)
            if np.sum(instance) < min_pixels:
                continue
            binary_masks.append(instance)
            labels.append(label)

    return binary_masks, labels

# Adds random Gaussian-like noise blobs to the input image to simulate noisy gene signals
def add_fake_noise(image, amount=0.01):
    img_np = np.array(image)
    noisy = img_np.copy()

    num_salt = int(amount * img_np.size * 0.5)
    num_pepper = int(amount * img_np.size * 0.5)

    coords = [np.random.randint(0, i - 1, num_salt) for i in img_np.shape[:2]]
    noisy[coords[0], coords[1]] = [255, 255, 255]

    coords = [np.random.randint(0, i - 1, num_pepper) for i in img_np.shape[:2]]
    noisy[coords[0], coords[1]] = [0, 0, 0]

    return Image.fromarray(noisy.astype(np.uint8))

# Removes small noisy regions from the predicted mask using morphological operations and area thresholding
def clean_mask(mask, min_area=20):
    if isinstance(mask, Image.Image):
        mask = mask.convert("RGB")
        mask = np.array(mask)

    binary_masks = []
    labels = []

    for color, label in COLOR_TO_LABEL.items():
        binary = np.all(mask == np.array(color), axis=-1).astype(np.uint8)
        labeled, num = cc_label(binary)
        for i in range(1, num + 1):
            instance = (labeled == i).astype(np.uint8)
            if instance.sum() >= min_area:
                binary_masks.append(instance)
                labels.append(label)

    return binary_masks, labels

# Returns the image transformation pipeline: converts to tensor and normalizes pixel values
def get_transforms(train=True):
    transforms = []
    transforms.append(T.ToTensor())
    if train:
        transforms.append(T.RandomHorizontalFlip(0.5))
        transforms.append(T.RandomApply([T.GaussianBlur(3)], p=0.3))
        transforms.append(T.RandomAdjustSharpness(sharpness_factor=2, p=0.3))
    return T.Compose(transforms)

"""## Custom Dataset"""

# Custom Dataset class to load gene images and their corresponding masks for instance segmentation
class GeneInstanceSegmentationDataset(Dataset):
    """
        Args:
            image_dir (str): Path to input images (RGB cell images).
            cell_mask_dir (str): Path to cell segmentation masks (used for adding noise).
            gene_mask_dir (str): Path to gene instance masks (color-coded).
            transforms (callable, optional): Optional image transformations.
    """
    def __init__(self, image_dir, cell_mask_dir, gene_mask_dir, transforms=None):
        self.image_dir = image_dir
        self.cell_mask_dir = cell_mask_dir
        self.gene_mask_dir = gene_mask_dir
        self.transforms = transforms
        self.image_files = sorted([f for f in os.listdir(image_dir) if f.endswith(".png")])

    def __len__(self):
        return len(self.image_files)

    # Loads and returns a single image and its corresponding target (masks, boxes, labels) after applying transformations
    def __getitem__(self, idx):
        filename = self.image_files[idx]
        img_path = os.path.join(self.image_dir, filename)
        cell_mask_path = os.path.join(self.cell_mask_dir, filename[:-9] + '.png')
        gene_mask_path = os.path.join(self.gene_mask_dir, filename[:-9] + '.png')

        image = np.array(Image.open(img_path).convert("RGB"))
        # print (image.size)
        cell_mask = np.array(Image.open(cell_mask_path).convert("RGB"))
        gene_mask = np.array(Image.open(gene_mask_path).convert("RGB"))

        # print('cell_mask', cell_mask.shape)
        # print('gene_mask', gene_mask.shape)

        unique_gene_masks, count = np.unique(gene_mask.reshape(-1, 3), axis=0, return_counts=True)
        count_sort_ind = np.argsort(-count)
        unique_gene_masks = unique_gene_masks[count_sort_ind]
        mask = np.all(gene_mask == unique_gene_masks[0], axis=-1)
        gene_mask[mask] = [0, 0, 0]
        mask = np.all(gene_mask == unique_gene_masks[1], axis=-1)
        gene_mask[mask] = [0, 0, 0]

        unique_cell_masks = np.unique(cell_mask.reshape(-1, 3), axis=0)
        unique_cell_masks = [c for c in unique_cell_masks if not np.all(c == 0)]

        cropped_images, cropped_cell_masks, cropped_gene_masks = [], [], []
        rois = []

        for unique_mask in unique_cell_masks:
            # Create a binary mask for each color
            mask = np.all(cell_mask == unique_mask, axis=-1)

            # print('unique_mask', unique_mask,'covered mask', mask.sum())

            if np.sum(mask) <= MIN_AREA:
                continue  # skip empty masks

            # Find bounding box for the mask
            y_indices, x_indices = np.where(mask)

            # print('y_indices', y_indices, 'x_indices', x_indices)
            y0, y1 = y_indices.min(), y_indices.max() + 1
            x0, x1 = x_indices.min(), x_indices.max() + 1

            # print ('For cell', unique_mask, 'coordinates are', y0, x0, 'and', y1, x1)

            rois.append([0, float(x0), float(y0), float(x1), float(y1)])

        img_tensor = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0
        cell_mask_tensor = torch.from_numpy(cell_mask).permute(2, 0, 1).float()
        gene_mask_tensor = torch.from_numpy(gene_mask).permute(2, 0, 1).float()


        if len(rois) == 0:
            print("No masks found.")
        else:
            # 3. Prepare image batch [N,C,H,W] and ROI tensor [num_rois,5]
            img_tensor = img_tensor.unsqueeze(0)  # [1,3,H,W]
            rois = torch.tensor(rois, dtype=torch.float32)

            # 4. Use ROIAlign - output fixed-size crops for each mask
            output_size = (64, 64)  # Change as needed
            image_patches = torchvision.ops.roi_align(img_tensor, rois, output_size=output_size)  # [num_masks, 3, 64, 64]
            # cell_mask_patches = torchvision.ops.roi_align(cell_mask_tensor.unsqueeze(0), rois, output_size=output_size)  # [num_masks, 3, 64, 64]
            gene_mask_patches = torchvision.ops.roi_pool(gene_mask_tensor.unsqueeze(0), rois, output_size=output_size)  # [num_masks, 3, 64, 64]
            # print(f"Extracted {image_patches.shape[0]} patches of shape {image_patches.shape[2:]}")


            # (Optional) Visualize the first patch
            # from torchvision.transforms.functional import to_pil_image
            # first_patch_img = to_pil_image(image_patches[0].cpu())
            # first_patch_cell_mask = to_pil_image(cell_mask_patches[0].cpu())
            # first_patch_gene_mask = to_pil_image(gene_mask_patches[0].cpu())

            # import matplotlib.pyplot as plt
            # fig, axes = plt.subplots(1, 3, figsize=(10, 15)) # 1 row, 2 columns

            # axes[0].imshow(first_patch_img)
            # axes[0].set_title("Image")
            # axes[0].axis('off') # Hide axes for cleaner display

            # axes[1].imshow(first_patch_cell_mask)
            # axes[1].set_title("Cell Mask")
            # axes[1].axis('off') # Hide axes for cleaner display

            # axes[2].imshow(first_patch_gene_mask)
            # axes[2].set_title("Gene Mask")
            # axes[2].axis('off') # Hide axes for cleaner display

            # plt.tight_layout() # Adjust subplot parameters for a tight layout
            # plt.savefig('/content/data/MyDrive/FISH_data/combined_image.jpg')

        targets = []

        for gene_mask_patch in gene_mask_patches:
            patch_target = get_patch_target(gene_mask_patch)
            targets.append(patch_target)
            torch.cuda.empty_cache()

        # del masks_tensor, labels_tensor, boxes, instance_masks, instance_labels
        

        return image_patches, gene_mask_patches, targets


from cc_torch import connected_components_labeling

def get_patch_target(patch_mask_tensor):
    """
    Returns: dict with masks, boxes, labels - all torch.Tensors
    """
    instance_masks = []
    instance_labels = []
    N, H, W = patch_mask_tensor.shape
    signal_classes = torch.unique(torch.reshape(patch_mask_tensor.permute(1, 2, 0), (-1, 3)), dim=0)

    # print('signal_classes', signal_classes)
    for signal_class in signal_classes:
        # print('signal_class', signal_class)
        if torch.equal(signal_class, torch.tensor([0, 128, 0])): #'green'
            label = 1
            color = 'green'
        elif torch.equal(signal_class, torch.tensor([128, 128, 0])): # 'yellow'
            label = 2
            color = 'yellow'
        elif torch.equal(signal_class, torch.tensor([0, 0, 128])): #'aqua'
            label = 3
            color = 'aqua'
        elif torch.equal(signal_class, torch.tensor([128, 0, 128])): #'fusion'
            label = 4
            color = 'fusion'
        elif torch.equal(signal_class, torch.tensor([128, 0, 0])): # 'red'
            label = 0
            color = 'red'
        else:
            continue

        mask = (patch_mask_tensor == signal_class.view(3, 1, 1)).all(dim=0)
        mask = mask.to(torch.uint8).to(DEVICE)
        # print('mask', mask.shape, torch.unique(mask, return_counts = True))

        if torch.sum(mask) < 1:
            continue

        connected_regions = connected_components_labeling(mask)
        # print('connected_regions', connected_regions.shape, torch.unique(connected_regions))
        # print('connected_regions', connected_regions)

        # from torchvision.transforms.functional import to_pil_image
        # connected_regions_img = to_pil_image(mask.to(torch.uint8).cpu())
        # patch_mask_tensor_img = to_pil_image(patch_mask_tensor.to(torch.uint8).cpu())
        # import matplotlib.pyplot as plt
        # fig, axes = plt.subplots(1, 2, figsize=(15, 15)) # 1 row, 1 columns
        # axes[0].imshow(mask)
        # axes[0].set_title("Image " + str(torch.unique(mask)))
        # axes[0].axis('off') # Hide axes for cleaner display

        # axes[1].imshow(patch_mask_tensor_img)
        # axes[1].set_title("Patch ")
        # axes[1].axis('off') # Hide axes for cleaner display

        # plt.tight_layout() # Adjust subplot parameters for a tight layout
        # plt.savefig('./mask_' + color + '_' + str(signal_class) + '.jpg')

        # (Optional) Visualize the patch
        # from torchvision.transforms.functional import to_pil_image
        # connected_regions_img = to_pil_image(connected_regions.cpu())
        # patch_mask_tensor_img = to_pil_image(patch_mask_tensor.to(torch.uint8).cpu())
        # import matplotlib.pyplot as plt
        # fig, axes = plt.subplots(1, 2, figsize=(15, 15)) # 1 row, 1 columns
        # axes[0].imshow(connected_regions_img)
        # axes[0].set_title("Image " + str(torch.unique(connected_regions)))
        # axes[0].axis('off') # Hide axes for cleaner display

        # axes[1].imshow(patch_mask_tensor_img)
        # axes[1].set_title("Patch ")
        # axes[1].axis('off') # Hide axes for cleaner display

        # plt.tight_layout() # Adjust subplot parameters for a tight layout
        # plt.savefig('./patch_image_' + color + '_' + str(signal_class) + '.jpg')


        for region_id in range(1, connected_regions.max().item() + 1):
            instance_mask = (connected_regions == region_id)

            if instance_mask.sum() > 2:
                instance_masks.append(instance_mask.to(torch.uint8))
                instance_labels.append(label)

                # from torchvision.transforms.functional import to_pil_image
                # instance_mask_img = to_pil_image(instance_mask.to(torch.uint8).cpu())
                # patch_mask_tensor_img = to_pil_image(patch_mask_tensor.to(torch.uint8).cpu())
                # import matplotlib.pyplot as plt
                # fig, axes = plt.subplots(1, 2, figsize=(15, 15)) # 1 row, 1 columns
                # axes[0].imshow(instance_mask_img)
                # axes[0].set_title("Image " + str(torch.unique(instance_mask)))
                # axes[0].axis('off') # Hide axes for cleaner display

                # axes[1].imshow(patch_mask_tensor_img)
                # axes[1].set_title("Patch ")
                # axes[1].axis('off') # Hide axes for cleaner display

                # plt.tight_layout() # Adjust subplot parameters for a tight layout
                # plt.savefig('./instance_mask_' + str(region_id) + '.jpg')

    if not instance_masks:
        return {'boxes': torch.zeros((0, 4)).to(DEVICE), 'labels': torch.zeros((0,), dtype=torch.long).to(DEVICE), 'masks': torch.zeros((0, H, W)).to(DEVICE)}

    masks_tensor = torch.stack(instance_masks).to(DEVICE)
    labels_tensor = torch.tensor(instance_labels, dtype=torch.int64, device=DEVICE)
    boxes = torchvision.ops.masks_to_boxes(masks_tensor.float())

    #prevent boxes with zero slices
    if not (boxes[:, 2] > boxes[:, 0]).all(): 
        indices_zero_xs = (boxes[:, 2] == boxes[:, 0])
        boxes[indices_zero_xs, 2] += 1

    if not (boxes[:, 3] > boxes[:, 1]).all():
        indices_zero_ys = (boxes[:, 3] == boxes[:, 1])
        boxes[indices_zero_ys, 3] += 1

    # print('boxes', boxes.shape, boxes)
    # print('labels_tensor', labels_tensor.shape, labels_tensor)
    # if torch.any(labels_tensor < 0):
    #     print('++++ LT zeros ++++', labels_tensor)
    # print('masks_tensor', masks_tensor.shape, masks_tensor)

    # if torch.cuda.is_available():
    #     print("In the get small patch function (after clearing cache)")
    #     print(f"Allocated memory (MBs): {torch.cuda.memory_allocated()/(1024**2)}")
    #     print(f"Cached memory (MBs): {torch.cuda.memory_reserved()/(1024**2)}")

    target = {
    "boxes": boxes.to(torch.uint8),         # torch.Tensor [N, 4]
    "labels": labels_tensor, # torch.Tensor [N]
    "masks": masks_tensor    # torch.Tensor [N, H, W]
    }

    del masks_tensor, labels_tensor, boxes, instance_masks, instance_labels
    torch.cuda.empty_cache()

    # if torch.cuda.is_available():
    #     print("In the small patch target function (after clearing cache)")
    #     print(f"Allocated memory (MBs): {torch.cuda.memory_allocated()/(1024**2)}")
    #     print(f"Cached memory (MBs): {torch.cuda.memory_reserved()/(1024**2)}")

    return target

"""## Model"""

# Returns a Mask R-CNN model customized for 4 gene classes with the specified number of input channels
def get_model(num_classes):
    model = maskrcnn_resnet50_fpn(weights="DEFAULT")
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)

    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels
    hidden_layer = 256
    model.roi_heads.mask_predictor = torchvision.models.detection.mask_rcnn.MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)
    return model

"""## Training"""

# Trains the Mask R-CNN model on the provided dataset using the specified hyperparameters and saves the trained model
def train_model(model, dataloader, num_epochs=10):
    model.to(DEVICE)
    model.train()
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

    for epoch in range(num_epochs):
        start = time.time()
        total_loss = 0.0

        print('******** training ***********')

        print(f"\nEpoch [{epoch+1}/{num_epochs}]")
        for images, targets in tqdm(dataloader, desc=f"Training Epoch {epoch+1}"):

            patch_images = []
            patch_targets = []
            for image, target in zip(images, targets):
                patch_images.extend([torch.squeeze(img).to(DEVICE) for img in image])  #img.squeeze().to(DEVICE)
                patch_targets.extend([{k: v.to(DEVICE) for k, v in t.items()} for t in target])
                torch.cuda.empty_cache()

            loss_dict = model(patch_images, patch_targets)
            losses = sum(loss for loss in loss_dict.values())
            total_loss += losses.item()

            print(f"Loss: {losses.item():.4f}, Time: {time.time() - start:.2f}s")

            # if torch.cuda.is_available():
            #     print("In the train function")
            #     print(f"Allocated memory (MBs): {torch.cuda.memory_allocated()/(1024**2)}")
            #     print(f"Cached memory (MBs): {torch.cuda.memory_reserved()/(1024**2)}")

            optimizer.zero_grad()
            losses.backward()
            optimizer.step()
            torch.save(model.state_dict(), './models/mask_rcnn_gene_model_sukrit_epoch_' + str(epoch) + '.pth')
        print(f"S_Total Loss: {total_loss:.4f}, Time: {time.time() - start:.2f}s")

        torch.save(model.state_dict(), './models/mask_rcnn_gene_model.pth')

        torch.cuda.empty_cache()
    print("\nTraining complete.")
    return model

"""## Execution"""

train_image_dir = r"/home/cvpr_ug_4/FISH/data/train_data_sample/images"
train_instance_mask_dir = r"/home/cvpr_ug_4/FISH/data/train_data_sample/masks"
train_gene_mask_dir = r"/home/cvpr_ug_4/FISH/data/train_data_sample/semantic_masks"

def collate_fn(batch):
    images, gene_patches, targets = zip(*batch)
    # MAX_PATCHES = MAX_PATCHES//BATCH_SIZE
    images = torch.cat(images, dim=0)
    gene_patches = torch.cat(gene_patches, dim=0)
    targets = sum(targets, [])

    # print('images, gene_patches, targets', images.shape, len(images), gene_patches.shape, len(gene_patches), targets, len(targets))
    num_patches = len(gene_patches)
    selected_indices = torch.randperm(num_patches)[:min(num_patches, MAX_PATCHES)]

    # print('Number of patches', num_patches, 'Selected Indices', selected_indices)

    gene_patches = gene_patches[selected_indices]
    targets = [targets[i] for i in selected_indices.tolist()]

    return [gene_patches], [targets]


def load_trained_model(model, model_path, num_classes=NUM_CLASSES):
    print('Loading exiting model at: ', model_path)
    model = get_model(num_classes)
    model.load_state_dict(torch.load(model_path, map_location=DEVICE))
    model.to(DEVICE)
    return model

dataset = GeneInstanceSegmentationDataset(train_image_dir, train_instance_mask_dir, train_gene_mask_dir, transforms=get_transforms(train=True))
data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)

model = get_model(NUM_CLASSES)

load_file_path = './models/mask_rcnn_gene_model_sukrit_epoch_17.pth'
file_path = "./models/mask_rcnn_gene_model.pth"

if os.path.exists(load_file_path):
    model = load_trained_model(model, load_file_path)

trained_model = train_model(model, data_loader, num_epochs=NUM_EPOCHS)

# Save model
torch.save(trained_model.state_dict(), file_path)

"""## Evaluate Test Dataset"""

# Extracts the count of each gene type (Red, Green, Fusion, Aqua) from a semantic mask image
def extract_counts_from_semantic_mask(mask_path):
    mask = np.array(Image.open(mask_path).convert("RGB"))
    raw_counts = {}

    for color, label in COLOR_TO_LABEL.items():
        binary = np.all(mask == np.array(color), axis=-1).astype(np.uint8)

        labeled, count = cc_label(binary)
        if count > 0:
            raw_counts[label] = count

    # Convert numeric labels to class names for comparison
    label_counts = {name: 0 for name in CLASS_COLORS.keys()}
    for label, count in raw_counts.items():
        class_name = LABEL_TO_CLASS.get(label, "unknown")
        if class_name in label_counts:
            label_counts[class_name] += count

    return label_counts

# Evaluates model predictions by comparing predicted gene counts with ground truth counts from semantic masks
def evaluate_with_ground_truth(model, test_dataset):
    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)
    model.eval()
    model.to(DEVICE)

    print("\nEvaluation against test ground truth:")
    with torch.no_grad():
        for images, targets in tqdm(test_loader):
            patch_images = []
            patch_targets = []
            for image, target in zip(images, targets):
                patch_images.extend([torch.squeeze(img).to(DEVICE) for img in image])  #img.squeeze().to(DEVICE)
                patch_targets.extend([{k: v.to(DEVICE) for k, v in t.items()} for t in target])
                torch.cuda.empty_cache()

            # Get predictions
            predictions = model(patch_images)
            
            for index, prediction in enumerate(predictions):
                pred_confidence = prediction["scores"] # [N] providing the confidence score for each detected object's class prediction.
                pred_labels = prediction["labels"] # [N] N is the number of detected instances in that image.
                pred_boxes = prediction["boxes"] #  [N, 4] N is the number of detected instances in that image.
                pred_masks = prediction["masks"] # [N, 1, H, W] H and W correspond to the height and width of the input image or a scaled version of it

                GT_confidence = target[index]["scores"]
                GT_labels = target[index]["labels"]
                GT_boxes = target[index]["boxes"] 
                GT_masks = target[index]["masks"]



            for i, label in enumerate(prediction["labels"]):
                if prediction["scores"][i] < 0.5:  # skip low confidence detections
                    continue
                cname = LABEL_TO_CLASS.get(label.item(), "unknown")
                if cname in pred_counts:
                    pred_counts[cname] += 1


            # Ground truth


            mask_path = os.path.join(test_dataset.mask_dir, fname)
            gt_counts = extract_counts_from_semantic_mask(mask_path)

            print(f"\n{fname}:")
            print(f"  Predicted: {dict(pred_counts)}")
            print(f"  Ground Truth: {gt_counts}")



test_image_dir = r"/home/cvpr_ug_4/FISH/data/test_data_sample/images"
test_instance_mask_dir = r"/home/cvpr_ug_4/FISH/data/test_data_sample/masks"
test_gene_mask_dir = r"/home/cvpr_ug_4/FISH/data/test_data_sample/semantic_masks"


model = get_model(NUM_CLASSES)
test_dataset = GeneInstanceSegmentationDataset(test_image_dir, test_gene_mask_dir)
model = load_trained_model(model, file_path)
evaluate_with_ground_truth(model, test_dataset)