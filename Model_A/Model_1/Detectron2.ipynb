{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python Kernel - Homebrew Python 3.9.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision opencv-python-headless detectron2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install git+https://github.com/facebookresearch/detectron2.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detectron2 version: 0.6\n",
      "Detectron2 installation path: /Users/niti/Library/Python/3.9/lib/python/site-packages/detectron2/__init__.py\n",
      "Looking for configs in: /Users/niti/Library/Python/3.9/lib/python/site-packages/detectron2/model_zoo/configs\n",
      "✅ Found Mask R-CNN config at: /Users/niti/Library/Python/3.9/lib/python/site-packages/detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\n"
     ]
    }
   ],
   "source": [
    "import detectron2\n",
    "import os\n",
    "print(f'Detectron2 version: {detectron2.__version__}')\n",
    "print(f'Detectron2 installation path: {detectron2.__file__}')\n",
    "\n",
    "# Check for config files\n",
    "cfg_dir = os.path.join(os.path.dirname(os.path.dirname(detectron2.__file__)), 'detectron2/model_zoo/configs')\n",
    "print(f'Looking for configs in: {cfg_dir}')\n",
    "\n",
    "# Check specifically for the Mask R-CNN config\n",
    "mask_rcnn_config = os.path.join(cfg_dir, 'COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml')\n",
    "if os.path.exists(mask_rcnn_config):\n",
    "    print(f'✅ Found Mask R-CNN config at: {mask_rcnn_config}')\n",
    "else:\n",
    "    print(f'❌ Mask R-CNN config not found at: {mask_rcnn_config}')\n",
    "    \n",
    "    # Try to find it elsewhere\n",
    "    import glob\n",
    "    configs = glob.glob(os.path.join(os.path.dirname(detectron2.__file__), '**', 'mask_rcnn_R_50_FPN_3x.yaml'), recursive=True)\n",
    "    if configs:\n",
    "        print(f'Found alternative config locations: {configs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "\n",
    "# Define folder paths\n",
    "folder_A = \"/Users/niti/Downloads/yolo_annotations/train/images\"\n",
    "folder_B = \"/Users/niti/Desktop/macOS_D/sem8/CP303/Compilation-Data-170325/FISH-All-Filtered/Cell-Only\"\n",
    "folder_C = \"/Users/niti/Desktop/macOS_D/sem8/CP303/FISH-Instance-Net/Cell-Dataset/masks\"\n",
    "folder_D = \"/Users/niti/Desktop/macOS_D/sem8/CP303/FISH-Instance-Net/Cell-Dataset/images\"\n",
    "\n",
    "\n",
    "# Ensure output folders exist\n",
    "os.makedirs(folder_C, exist_ok=True)\n",
    "os.makedirs(folder_D, exist_ok=True)\n",
    "\n",
    "# Get base names (without extension) and their corresponding extensions from folder A\n",
    "image_extensions = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"}\n",
    "files_A = {os.path.splitext(f)[0]: os.path.splitext(f)[1].lower() for f in os.listdir(folder_A) if os.path.splitext(f)[1].lower() in image_extensions}\n",
    "\n",
    "# Copy images from A to C\n",
    "for base_name, ext in files_A.items():\n",
    "    shutil.copy(os.path.join(folder_A, base_name + ext), os.path.join(folder_C, base_name + ext))\n",
    "\n",
    "# Copy matching base names from B to D and convert format\n",
    "for file in os.listdir(folder_B):\n",
    "    base_name, ext_B = os.path.splitext(file)\n",
    "    if base_name in files_A:  # Check if the base name exists in folder A\n",
    "        src_path_B = os.path.join(folder_B, file)\n",
    "        dest_ext_A = files_A[base_name]  # Get the extension from Folder A\n",
    "        dest_path_D = os.path.join(folder_D, base_name + dest_ext_A)\n",
    "\n",
    "        # Convert the file to the format of Folder A\n",
    "        with Image.open(src_path_B) as img:\n",
    "            img = img.convert(\"RGB\") if dest_ext_A in {\".jpg\", \".jpeg\"} else img  # Ensure proper conversion\n",
    "            img.save(dest_path_D, format=dest_ext_A.strip(\".\"))  # Save in the required format\n",
    "\n",
    "print(\"Files copied and converted successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files copied and converted successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "\n",
    "# Define folder paths\n",
    "folder_A = \"/Users/niti/Downloads/yolo_annotations/valid/images\"\n",
    "folder_B = \"/Users/niti/Desktop/macOS_D/sem8/CP303/Compilation-Data-170325/FISH-All-Filtered/Cell-Only\"\n",
    "folder_C = \"/Users/niti/Desktop/macOS_D/sem8/CP303/FISH-Instance-Net/Test-Dataset/masks\"\n",
    "folder_D = \"/Users/niti/Desktop/macOS_D/sem8/CP303/FISH-Instance-Net/Test-Dataset/images\"\n",
    "\n",
    "\n",
    "# Ensure output folders exist\n",
    "os.makedirs(folder_C, exist_ok=True)\n",
    "os.makedirs(folder_D, exist_ok=True)\n",
    "\n",
    "# Get base names (without extension) and their corresponding extensions from folder A\n",
    "image_extensions = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"}\n",
    "files_A = {os.path.splitext(f)[0]: os.path.splitext(f)[1].lower() for f in os.listdir(folder_A) if os.path.splitext(f)[1].lower() in image_extensions}\n",
    "\n",
    "# Copy images from A to C\n",
    "for base_name, ext in files_A.items():\n",
    "    shutil.copy(os.path.join(folder_A, base_name + ext), os.path.join(folder_C, base_name + ext))\n",
    "\n",
    "# Copy matching base names from B to D and convert format\n",
    "for file in os.listdir(folder_B):\n",
    "    base_name, ext_B = os.path.splitext(file)\n",
    "    if base_name in files_A:  # Check if the base name exists in folder A\n",
    "        src_path_B = os.path.join(folder_B, file)\n",
    "        dest_ext_A = files_A[base_name]  # Get the extension from Folder A\n",
    "        dest_path_D = os.path.join(folder_D, base_name + dest_ext_A)\n",
    "\n",
    "        # Convert the file to the format of Folder A\n",
    "        with Image.open(src_path_B) as img:\n",
    "            img = img.convert(\"RGB\") if dest_ext_A in {\".jpg\", \".jpeg\"} else img  # Ensure proper conversion\n",
    "            img.save(dest_path_D, format=dest_ext_A.strip(\".\"))  # Save in the required format\n",
    "\n",
    "print(\"Files copied and converted successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detectron2 Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import build_detection_test_loader\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.engine import DefaultPredictor\n",
    "import random\n",
    "\n",
    "def extract_instance_masks(image):\n",
    "    \"\"\"\n",
    "    Extract instance masks from a color-coded segmentation image\n",
    "    \n",
    "    Args:\n",
    "    image (numpy.ndarray): Color-coded segmentation image\n",
    "    \n",
    "    Returns:\n",
    "    list: List of binary masks for each unique color (excluding black)\n",
    "    \"\"\"\n",
    "    # Convert to uint8 if not already\n",
    "    image = image.astype(np.uint8)\n",
    "    \n",
    "    # Get unique colors (excluding black)\n",
    "    unique_colors = np.unique(image.reshape(-1, image.shape[2]), axis=0)\n",
    "    unique_colors = [color for color in unique_colors if not np.array_equal(color, [0,0,0])]\n",
    "    \n",
    "    # Extract masks\n",
    "    masks = []\n",
    "    for color in unique_colors:\n",
    "        # Create binary mask for this color\n",
    "        mask = np.all(image == color, axis=-1)\n",
    "        masks.append(mask)\n",
    "    \n",
    "    return masks\n",
    "\n",
    "def get_cell_dicts(img_dir):\n",
    "    \"\"\"\n",
    "    Parse cell segmentation dataset with color-coded masks\n",
    "    \n",
    "    Expected directory structure:\n",
    "    cell_dataset/\n",
    "    ├── images/\n",
    "    │   ├── image1.png\n",
    "    └── masks/\n",
    "        ├── image1.png\n",
    "    \"\"\"\n",
    "    dataset_dicts = []\n",
    "    \n",
    "    # Iterate through image files\n",
    "    for img_filename in sorted(os.listdir(os.path.join(img_dir, 'images'))):\n",
    "        if not img_filename.endswith(('.png', '.jpg', '.tif')):\n",
    "            continue\n",
    "        \n",
    "        # Full paths\n",
    "        img_path = os.path.join(img_dir, 'images', img_filename)\n",
    "        mask_path = os.path.join(img_dir, 'masks', img_filename)\n",
    "        \n",
    "        # Read image and mask\n",
    "        image = cv2.imread(img_path)\n",
    "        mask_image = cv2.imread(mask_path)\n",
    "        \n",
    "        # Get image dimensions\n",
    "        height, width = image.shape[:2]\n",
    "        \n",
    "        # Record for this image\n",
    "        record = {\n",
    "            \"file_name\": img_path,\n",
    "            \"image_id\": img_filename,\n",
    "            \"height\": height,\n",
    "            \"width\": width,\n",
    "            \"annotations\": []\n",
    "        }\n",
    "        \n",
    "        # Extract instance masks\n",
    "        masks = extract_instance_masks(mask_image)\n",
    "        \n",
    "        # Process each mask\n",
    "        for mask in masks:\n",
    "            # Find contours\n",
    "            contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            \n",
    "            # Get polygon representation\n",
    "            polygon = contours[0].flatten().tolist()\n",
    "            \n",
    "            # Compute bounding box\n",
    "            x, y, w, h = cv2.boundingRect(mask.astype(np.uint8))\n",
    "            \n",
    "            # Create annotation\n",
    "            annotation = {\n",
    "                \"bbox\": [x, y, x+w, y+h],\n",
    "                \"bbox_mode\": BoxMode.XYXY_ABS,\n",
    "                \"segmentation\": [polygon],\n",
    "                \"category_id\": 0,  # Assuming single cell class\n",
    "                \"iscrowd\": 0\n",
    "            }\n",
    "            \n",
    "            record[\"annotations\"].append(annotation)\n",
    "        \n",
    "        dataset_dicts.append(record)\n",
    "    \n",
    "    return dataset_dicts\n",
    "\n",
    "# Rest of the code remains the same as in the previous notebook\n",
    "def register_cell_dataset(dataset_dir):\n",
    "    \"\"\"Register the cell dataset with Detectron2\"\"\"\n",
    "    # Training dataset\n",
    "    DatasetCatalog.register(\"cell_train\", lambda d=dataset_dir: get_cell_dicts(d))\n",
    "    MetadataCatalog.get(\"cell_train\").set(thing_classes=[\"cell\"])\n",
    "    \n",
    "    # Validation dataset (if you have a separate validation set)\n",
    "    DatasetCatalog.register(\"cell_val\", lambda d=dataset_dir: get_cell_dicts(d))\n",
    "    MetadataCatalog.get(\"cell_val\").set(thing_classes=[\"cell\"])\n",
    "\n",
    "def setup_cfg(num_classes=1, output_dir='./output'):\n",
    "    \"\"\"Create configuration for Mask R-CNN\"\"\"\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(\"detectron2/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "    \n",
    "    # Modifications for your specific dataset\n",
    "    cfg.DATASETS.TRAIN = (\"cell_train\",)\n",
    "    cfg.DATASETS.TEST = (\"cell_val\",)\n",
    "    cfg.DATALOADER.NUM_WORKERS = 2\n",
    "    \n",
    "    # Pretrained weights\n",
    "    cfg.MODEL.WEIGHTS = \"detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\"\n",
    "    \n",
    "    # Number of classes\n",
    "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = num_classes\n",
    "    \n",
    "    # Output directory\n",
    "    cfg.OUTPUT_DIR = output_dir\n",
    "    \n",
    "    return cfg\n",
    "\n",
    "# Visualization function\n",
    "def visualize_dataset(dataset_dir):\n",
    "    \"\"\"\n",
    "    Visualize the dataset to verify mask extraction\n",
    "    \"\"\"\n",
    "    # Get dataset dictionary\n",
    "    dataset_dicts = get_cell_dicts(dataset_dir)\n",
    "    \n",
    "    # Visualize a few samples\n",
    "    for d in random.sample(dataset_dicts, min(3, len(dataset_dicts))):\n",
    "        img = cv2.imread(d[\"file_name\"])\n",
    "        \n",
    "        # Create visualizer\n",
    "        visualizer = Visualizer(img[:, :, ::-1], \n",
    "                                metadata=MetadataCatalog.get(\"cell_train\"), \n",
    "                                scale=1.2)\n",
    "        \n",
    "        # Draw annotations\n",
    "        vis = visualizer.draw_dataset_dict(d)\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.imshow(vis.get_image())\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Dataset Sample: {d['file_name']}\")\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dataset directory\n",
    "dataset_directory2 = \"/Users/vipulpatil/Desktop/FISH-Instance-Net/Cell-Dataset\"\n",
    "\n",
    "# Visualize dataset first to verify masks\n",
    "# visualize_dataset(dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Config for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_cfg(num_classes=1, output_dir='./output'):\n",
    "    \"\"\"Create configuration for Mask R-CNN with correct config path\"\"\"\n",
    "    from detectron2.config import get_cfg\n",
    "    \n",
    "    # Create configuration\n",
    "    cfg = get_cfg()\n",
    "    \n",
    "    # Use the exact path you verified exists\n",
    "    cfg_path = \"/Users/niti/Library/Python/3.9/lib/python/site-packages/detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"\n",
    "    cfg.merge_from_file(cfg_path)\n",
    "    \n",
    "    # Alternatively, use model_zoo which handles paths internally\n",
    "    # from detectron2 import model_zoo\n",
    "    # cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "    \n",
    "    # Modifications for your specific dataset\n",
    "    cfg.DATASETS.TRAIN = (\"cell_train\",)\n",
    "    cfg.DATASETS.TEST = (\"cell_val\",)\n",
    "    cfg.DATALOADER.NUM_WORKERS = 2\n",
    "    \n",
    "    # Pretrained weights\n",
    "    cfg.MODEL.WEIGHTS = \"detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\"\n",
    "    \n",
    "    # Number of classes\n",
    "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = num_classes\n",
    "    \n",
    "    # Output directory\n",
    "    cfg.OUTPUT_DIR = output_dir\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    cfg.SOLVER.IMS_PER_BATCH = 4\n",
    "    cfg.SOLVER.BASE_LR = 0.00025\n",
    "    cfg.SOLVER.MAX_ITER = 1000\n",
    "    cfg.SOLVER.STEPS = (700, 900)\n",
    "    \n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Config for CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_cfg(num_classes=1, output_dir='./output'):\n",
    "    \"\"\"Create configuration for Mask R-CNN with correct config path\"\"\"\n",
    "    from detectron2.config import get_cfg\n",
    "    \n",
    "    # Create configuration\n",
    "    cfg = get_cfg()\n",
    "    \n",
    "    # Use the exact path you verified exists\n",
    "    cfg_path = \"/opt/anaconda3/envs/detectron2-env/lib/python3.9/site-packages/detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"\n",
    "    cfg.merge_from_file(cfg_path)\n",
    "    \n",
    "    # Alternatively, use model_zoo which handles paths internally\n",
    "    # from detectron2 import model_zoo\n",
    "    # cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "    \n",
    "    # Force CPU\n",
    "    cfg.MODEL.DEVICE = \"cpu\"\n",
    "    \n",
    "    # Modifications for your specific dataset\n",
    "    cfg.DATASETS.TRAIN = (\"cell_train\",)\n",
    "    cfg.DATASETS.TEST = (\"cell_val\",)\n",
    "    cfg.DATALOADER.NUM_WORKERS = 2\n",
    "    \n",
    "    # Pretrained weights\n",
    "    cfg.MODEL.WEIGHTS = \"detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\"\n",
    "    \n",
    "    # Number of classes\n",
    "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = num_classes\n",
    "    \n",
    "    # Output directory\n",
    "    cfg.OUTPUT_DIR = output_dir\n",
    "    \n",
    "    # Training hyperparameters for CPU\n",
    "    # Reduce batch size for CPU training\n",
    "    cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "    # Lower learning rate for more stable CPU training\n",
    "    cfg.SOLVER.BASE_LR = 0.0001\n",
    "    # Reduce iterations for CPU (can be increased if needed)\n",
    "    cfg.SOLVER.MAX_ITER = 500\n",
    "    cfg.SOLVER.STEPS = (350, 450)\n",
    "    \n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellTrainer(DefaultTrainer):\n",
    "    \"\"\"Custom trainer with COCO evaluation\"\"\"\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name):\n",
    "        return COCOEvaluator(dataset_name, output_dir=cfg.OUTPUT_DIR)\n",
    "\n",
    "def train_instance_segmentation(dataset_dir):\n",
    "    \"\"\"\n",
    "    Complete training pipeline for instance segmentation\n",
    "    \n",
    "    Args:\n",
    "    dataset_dir (str): Path to the dataset directory\n",
    "    \"\"\"\n",
    "    # 1. Register dataset\n",
    "    register_cell_dataset(dataset_dir)\n",
    "    \n",
    "    # 2. Setup configuration\n",
    "    cfg = setup_cfg(\n",
    "        num_classes=1,  # Adjust if you have multiple cell types\n",
    "        output_dir='./cell_segmentation_output'\n",
    "    )\n",
    "    \n",
    "    # 3. Configure training hyperparameters for 500 iterations\n",
    "    cfg.SOLVER.IMS_PER_BATCH = 4  # This is fine if your GPU can handle it\n",
    "    cfg.SOLVER.BASE_LR = 0.0001  # Slightly higher learning rate for shorter training\n",
    "    cfg.SOLVER.MAX_ITER = 500  # Your desired iteration count\n",
    "    cfg.SOLVER.STEPS = (350, 450)  # Learning rate reduction at 70% and 90% of training\n",
    "    cfg.SOLVER.GAMMA = 0.1  # Learning rate reduction factor\n",
    "    cfg.SOLVER.WARMUP_FACTOR = 1.0 / 1000\n",
    "    cfg.SOLVER.WARMUP_ITERS = 50  # Warm up for first 10% of training\n",
    "    cfg.SOLVER.WEIGHT_DECAY = 0.0001  # Regularization to prevent overfitting\n",
    "    \n",
    "    # 4. Create output directory\n",
    "    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    # 5. Create trainer\n",
    "    trainer = CellTrainer(cfg)\n",
    "    \n",
    "    # 6. Resume or start training\n",
    "    trainer.resume_or_load(resume=False)\n",
    "    \n",
    "    # 7. Start training\n",
    "    trainer.train()\n",
    "    \n",
    "    # 8. Run inference on test set\n",
    "    # Optional: you can add test set evaluation here\n",
    "    try:\n",
    "        # Load the best model\n",
    "        cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "        cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7\n",
    "        predictor = DefaultPredictor(cfg)\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        evaluator = COCOEvaluator(\"cell_val\", output_dir=cfg.OUTPUT_DIR)\n",
    "        val_loader = build_detection_test_loader(cfg, \"cell_val\")\n",
    "        inference_results = inference_on_dataset(predictor.model, val_loader, evaluator)\n",
    "        print(\"Validation Results:\", inference_results)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Inference evaluation failed: {e}\")\n",
    "    \n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[05/11 19:19:41 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n",
      "    )\n",
      "    (mask_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (mask_head): MaskRCNNConvUpsampleHead(\n",
      "      (mask_fcn1): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn2): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn3): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn4): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (deconv_relu): ReLU()\n",
      "      (predictor): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m[05/11 19:21:09 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 209 images left.\n",
      "\u001b[32m[05/11 19:21:09 d2.data.build]: \u001b[0mDistribution of instances among all 1 categories:\n",
      "\u001b[36m|  category  | #instances   |\n",
      "|:----------:|:-------------|\n",
      "|    cell    | 2696         |\n",
      "|            |              |\u001b[0m\n",
      "\u001b[32m[05/11 19:21:09 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[05/11 19:21:09 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[05/11 19:21:09 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[05/11 19:21:09 d2.data.common]: \u001b[0mSerializing 209 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[05/11 19:21:09 d2.data.common]: \u001b[0mSerialized dataset takes 2.03 MiB\n",
      "\u001b[32m[05/11 19:21:09 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=4\n",
      "\u001b[32m[05/11 19:21:09 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (4, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (1, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (1,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.predictor.{bias, weight}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[05/11 19:21:09 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/detectron2-env/lib/python3.9/site-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3638.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[05/11 19:28:25 d2.utils.events]: \u001b[0m eta: 2:59:24  iter: 19  total_loss: 2.224  loss_cls: 0.7091  loss_box_reg: 0.6052  loss_mask: 0.6775  loss_rpn_cls: 0.1869  loss_rpn_loc: 0.02842    time: 21.9660  last_time: 27.0864  data_time: 0.1158  last_data_time: 0.0043   lr: 3.8062e-05  \n",
      "\u001b[32m[05/11 19:35:50 d2.utils.events]: \u001b[0m eta: 2:51:56  iter: 39  total_loss: 1.859  loss_cls: 0.443  loss_box_reg: 0.7077  loss_mask: 0.5706  loss_rpn_cls: 0.07468  loss_rpn_loc: 0.0394    time: 22.0904  last_time: 23.4827  data_time: 0.0178  last_data_time: 0.0780   lr: 7.8022e-05  \n",
      "\u001b[32m[05/11 19:43:13 d2.utils.events]: \u001b[0m eta: 2:46:10  iter: 59  total_loss: 1.494  loss_cls: 0.3471  loss_box_reg: 0.6427  loss_mask: 0.4364  loss_rpn_cls: 0.04792  loss_rpn_loc: 0.03718    time: 22.1195  last_time: 22.4970  data_time: 0.0282  last_data_time: 0.0138   lr: 0.0001  \n",
      "\u001b[32m[05/11 19:50:40 d2.utils.events]: \u001b[0m eta: 2:36:43  iter: 79  total_loss: 1.337  loss_cls: 0.3021  loss_box_reg: 0.6454  loss_mask: 0.3594  loss_rpn_cls: 0.0115  loss_rpn_loc: 0.03137    time: 22.1768  last_time: 22.4978  data_time: 0.0174  last_data_time: 0.0038   lr: 0.0001  \n",
      "\u001b[32m[05/11 19:58:00 d2.utils.events]: \u001b[0m eta: 2:28:36  iter: 99  total_loss: 1.26  loss_cls: 0.2603  loss_box_reg: 0.6467  loss_mask: 0.2963  loss_rpn_cls: 0.008064  loss_rpn_loc: 0.02653    time: 22.1359  last_time: 22.2873  data_time: 0.0179  last_data_time: 0.0077   lr: 0.0001  \n",
      "\u001b[32m[05/11 20:05:29 d2.utils.events]: \u001b[0m eta: 2:21:10  iter: 119  total_loss: 1.097  loss_cls: 0.2266  loss_box_reg: 0.581  loss_mask: 0.2621  loss_rpn_cls: 0.007169  loss_rpn_loc: 0.02632    time: 22.1899  last_time: 19.6257  data_time: 0.0197  last_data_time: 0.0104   lr: 0.0001  \n",
      "\u001b[32m[05/11 20:12:49 d2.utils.events]: \u001b[0m eta: 2:13:41  iter: 139  total_loss: 0.9809  loss_cls: 0.1985  loss_box_reg: 0.5202  loss_mask: 0.2259  loss_rpn_cls: 0.008631  loss_rpn_loc: 0.03202    time: 22.1641  last_time: 23.6598  data_time: 0.0200  last_data_time: 0.0044   lr: 0.0001  \n",
      "\u001b[32m[05/11 20:20:16 d2.utils.events]: \u001b[0m eta: 2:05:52  iter: 159  total_loss: 0.8495  loss_cls: 0.1649  loss_box_reg: 0.4256  loss_mask: 0.2044  loss_rpn_cls: 0.005969  loss_rpn_loc: 0.02293    time: 22.1882  last_time: 24.9456  data_time: 0.0178  last_data_time: 0.0202   lr: 0.0001  \n",
      "\u001b[32m[05/11 20:28:16 d2.utils.events]: \u001b[0m eta: 1:58:51  iter: 179  total_loss: 0.7683  loss_cls: 0.1701  loss_box_reg: 0.3719  loss_mask: 0.1825  loss_rpn_cls: 0.005802  loss_rpn_loc: 0.0335    time: 22.3893  last_time: 21.8494  data_time: 0.0250  last_data_time: 0.0038   lr: 0.0001  \n",
      "\u001b[32m[05/11 20:35:15 d2.utils.events]: \u001b[0m eta: 1:51:03  iter: 199  total_loss: 0.61  loss_cls: 0.1354  loss_box_reg: 0.2675  loss_mask: 0.157  loss_rpn_cls: 0.004298  loss_rpn_loc: 0.02496    time: 22.2469  last_time: 20.8482  data_time: 0.0202  last_data_time: 0.0046   lr: 0.0001  \n",
      "\u001b[32m[05/11 20:42:47 d2.utils.events]: \u001b[0m eta: 1:43:09  iter: 219  total_loss: 0.5117  loss_cls: 0.1136  loss_box_reg: 0.2323  loss_mask: 0.1558  loss_rpn_cls: 0.002832  loss_rpn_loc: 0.01609    time: 22.2782  last_time: 20.4397  data_time: 0.0235  last_data_time: 0.0526   lr: 0.0001  \n",
      "\u001b[32m[05/11 20:50:43 d2.utils.events]: \u001b[0m eta: 1:36:13  iter: 239  total_loss: 0.6073  loss_cls: 0.1327  loss_box_reg: 0.2513  loss_mask: 0.155  loss_rpn_cls: 0.006606  loss_rpn_loc: 0.03393    time: 22.4058  last_time: 21.8158  data_time: 0.0226  last_data_time: 0.0241   lr: 0.0001  \n",
      "\u001b[32m[05/11 20:57:56 d2.utils.events]: \u001b[0m eta: 1:28:11  iter: 259  total_loss: 0.4858  loss_cls: 0.1067  loss_box_reg: 0.1867  loss_mask: 0.1506  loss_rpn_cls: 0.005305  loss_rpn_loc: 0.02457    time: 22.3451  last_time: 18.7242  data_time: 0.0198  last_data_time: 0.0034   lr: 0.0001  \n",
      "\u001b[32m[05/11 21:05:25 d2.utils.events]: \u001b[0m eta: 1:20:42  iter: 279  total_loss: 0.4849  loss_cls: 0.103  loss_box_reg: 0.1945  loss_mask: 0.1389  loss_rpn_cls: 0.006403  loss_rpn_loc: 0.02455    time: 22.3549  last_time: 23.0930  data_time: 0.0205  last_data_time: 0.0050   lr: 0.0001  \n",
      "\u001b[32m[05/11 21:11:46 d2.utils.events]: \u001b[0m eta: 1:12:43  iter: 299  total_loss: 0.5071  loss_cls: 0.1185  loss_box_reg: 0.201  loss_mask: 0.1512  loss_rpn_cls: 0.004173  loss_rpn_loc: 0.02155    time: 22.1307  last_time: 14.0620  data_time: 0.0205  last_data_time: 0.0037   lr: 0.0001  \n",
      "\u001b[32m[05/11 21:18:39 d2.utils.events]: \u001b[0m eta: 1:05:19  iter: 319  total_loss: 0.5517  loss_cls: 0.127  loss_box_reg: 0.1979  loss_mask: 0.1513  loss_rpn_cls: 0.001795  loss_rpn_loc: 0.02247    time: 22.0377  last_time: 18.1821  data_time: 0.0215  last_data_time: 0.0272   lr: 0.0001  \n",
      "\u001b[32m[05/11 21:25:01 d2.utils.events]: \u001b[0m eta: 0:57:43  iter: 339  total_loss: 0.4868  loss_cls: 0.098  loss_box_reg: 0.183  loss_mask: 0.166  loss_rpn_cls: 0.004565  loss_rpn_loc: 0.02774    time: 21.8636  last_time: 20.8495  data_time: 0.0176  last_data_time: 0.0516   lr: 0.0001  \n",
      "\u001b[32m[05/11 21:31:48 d2.utils.events]: \u001b[0m eta: 0:50:22  iter: 359  total_loss: 0.4348  loss_cls: 0.09669  loss_box_reg: 0.1679  loss_mask: 0.1424  loss_rpn_cls: 0.00364  loss_rpn_loc: 0.0218    time: 21.7807  last_time: 19.3907  data_time: 0.0188  last_data_time: 0.0191   lr: 1e-05  \n",
      "\u001b[32m[05/11 21:37:42 d2.utils.events]: \u001b[0m eta: 0:42:37  iter: 379  total_loss: 0.492  loss_cls: 0.1088  loss_box_reg: 0.1986  loss_mask: 0.1388  loss_rpn_cls: 0.004375  loss_rpn_loc: 0.02209    time: 21.5627  last_time: 19.3003  data_time: 0.0153  last_data_time: 0.0555   lr: 1e-05  \n",
      "\u001b[32m[05/11 21:44:13 d2.utils.events]: \u001b[0m eta: 0:35:28  iter: 399  total_loss: 0.4161  loss_cls: 0.08592  loss_box_reg: 0.1656  loss_mask: 0.1376  loss_rpn_cls: 0.003044  loss_rpn_loc: 0.02283    time: 21.4626  last_time: 22.1440  data_time: 0.0201  last_data_time: 0.0076   lr: 1e-05  \n",
      "\u001b[32m[05/11 21:49:47 d2.utils.events]: \u001b[0m eta: 0:28:03  iter: 419  total_loss: 0.4073  loss_cls: 0.09106  loss_box_reg: 0.1631  loss_mask: 0.1368  loss_rpn_cls: 0.005214  loss_rpn_loc: 0.02401    time: 21.2336  last_time: 12.8017  data_time: 0.0151  last_data_time: 0.0308   lr: 1e-05  \n",
      "\u001b[32m[05/11 21:55:50 d2.utils.events]: \u001b[0m eta: 0:20:51  iter: 439  total_loss: 0.5144  loss_cls: 0.1091  loss_box_reg: 0.1945  loss_mask: 0.1551  loss_rpn_cls: 0.004422  loss_rpn_loc: 0.02036    time: 21.0944  last_time: 18.5687  data_time: 0.0176  last_data_time: 0.0457   lr: 1e-05  \n",
      "\u001b[32m[05/11 22:01:57 d2.utils.events]: \u001b[0m eta: 0:13:51  iter: 459  total_loss: 0.4592  loss_cls: 0.1111  loss_box_reg: 0.1871  loss_mask: 0.1488  loss_rpn_cls: 0.001919  loss_rpn_loc: 0.02497    time: 20.9746  last_time: 15.1734  data_time: 0.0212  last_data_time: 0.0025   lr: 1e-06  \n",
      "\u001b[32m[05/11 22:08:05 d2.utils.events]: \u001b[0m eta: 0:06:52  iter: 479  total_loss: 0.4715  loss_cls: 0.1043  loss_box_reg: 0.1988  loss_mask: 0.1433  loss_rpn_cls: 0.005848  loss_rpn_loc: 0.02793    time: 20.8652  last_time: 25.6706  data_time: 0.0173  last_data_time: 0.0059   lr: 1e-06  \n",
      "\u001b[32m[05/11 22:14:44 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 499  total_loss: 0.5007  loss_cls: 0.1071  loss_box_reg: 0.1891  loss_mask: 0.1454  loss_rpn_cls: 0.007304  loss_rpn_loc: 0.02821    time: 20.8276  last_time: 22.6727  data_time: 0.0164  last_data_time: 0.0022   lr: 1e-06  \n",
      "\u001b[32m[05/11 22:14:44 d2.engine.hooks]: \u001b[0mOverall training speed: 498 iterations in 2:52:52 (20.8277 s / it)\n",
      "\u001b[32m[05/11 22:14:44 d2.engine.hooks]: \u001b[0mTotal training time: 2:52:54 (0:00:01 on hooks)\n",
      "\u001b[32m[05/11 22:16:14 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[05/11 22:16:14 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[05/11 22:16:14 d2.data.common]: \u001b[0mSerializing 209 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[05/11 22:16:14 d2.data.common]: \u001b[0mSerialized dataset takes 2.03 MiB\n",
      "\u001b[32m[05/11 22:16:14 d2.evaluation.coco_evaluation]: \u001b[0mTrying to convert 'cell_val' to COCO format ...\n",
      "\u001b[32m[05/11 22:16:14 d2.data.datasets.coco]: \u001b[0mConverting annotations of dataset 'cell_val' to COCO format ...)\n",
      "\u001b[32m[05/11 22:17:42 d2.data.datasets.coco]: \u001b[0mConverting dataset dicts into COCO format\n",
      "\u001b[32m[05/11 22:17:42 d2.data.datasets.coco]: \u001b[0mConversion finished, #images: 209, #annotations: 2696\n",
      "\u001b[32m[05/11 22:17:42 d2.data.datasets.coco]: \u001b[0mCaching COCO format annotations at './cell_segmentation_output/cell_val_coco_format.json' ...\n",
      "\u001b[32m[05/11 22:17:43 d2.evaluation.evaluator]: \u001b[0mStart inference on 209 batches\n",
      "\u001b[32m[05/11 22:18:00 d2.evaluation.evaluator]: \u001b[0mInference done 11/209. Dataloading: 0.0007 s/iter. Inference: 1.3281 s/iter. Eval: 0.0128 s/iter. Total: 1.3416 s/iter. ETA=0:04:25\n",
      "\u001b[32m[05/11 22:18:05 d2.evaluation.evaluator]: \u001b[0mInference done 15/209. Dataloading: 0.0010 s/iter. Inference: 1.3329 s/iter. Eval: 0.0126 s/iter. Total: 1.3468 s/iter. ETA=0:04:21\n",
      "\u001b[32m[05/11 22:18:11 d2.evaluation.evaluator]: \u001b[0mInference done 19/209. Dataloading: 0.0013 s/iter. Inference: 1.3442 s/iter. Eval: 0.0122 s/iter. Total: 1.3580 s/iter. ETA=0:04:18\n",
      "\u001b[32m[05/11 22:18:16 d2.evaluation.evaluator]: \u001b[0mInference done 24/209. Dataloading: 0.0012 s/iter. Inference: 1.2719 s/iter. Eval: 0.0110 s/iter. Total: 1.2845 s/iter. ETA=0:03:57\n",
      "\u001b[32m[05/11 22:18:22 d2.evaluation.evaluator]: \u001b[0mInference done 28/209. Dataloading: 0.0012 s/iter. Inference: 1.2808 s/iter. Eval: 0.0141 s/iter. Total: 1.2965 s/iter. ETA=0:03:54\n",
      "\u001b[32m[05/11 22:18:27 d2.evaluation.evaluator]: \u001b[0mInference done 32/209. Dataloading: 0.0011 s/iter. Inference: 1.2869 s/iter. Eval: 0.0129 s/iter. Total: 1.3013 s/iter. ETA=0:03:50\n",
      "\u001b[32m[05/11 22:18:33 d2.evaluation.evaluator]: \u001b[0mInference done 37/209. Dataloading: 0.0010 s/iter. Inference: 1.2675 s/iter. Eval: 0.0115 s/iter. Total: 1.2805 s/iter. ETA=0:03:40\n",
      "\u001b[32m[05/11 22:18:39 d2.evaluation.evaluator]: \u001b[0mInference done 42/209. Dataloading: 0.0010 s/iter. Inference: 1.2570 s/iter. Eval: 0.0104 s/iter. Total: 1.2689 s/iter. ETA=0:03:31\n",
      "\u001b[32m[05/11 22:18:44 d2.evaluation.evaluator]: \u001b[0mInference done 46/209. Dataloading: 0.0010 s/iter. Inference: 1.2591 s/iter. Eval: 0.0121 s/iter. Total: 1.2726 s/iter. ETA=0:03:27\n",
      "\u001b[32m[05/11 22:18:50 d2.evaluation.evaluator]: \u001b[0mInference done 51/209. Dataloading: 0.0010 s/iter. Inference: 1.2534 s/iter. Eval: 0.0116 s/iter. Total: 1.2664 s/iter. ETA=0:03:20\n",
      "\u001b[32m[05/11 22:18:56 d2.evaluation.evaluator]: \u001b[0mInference done 55/209. Dataloading: 0.0010 s/iter. Inference: 1.2770 s/iter. Eval: 0.0116 s/iter. Total: 1.2900 s/iter. ETA=0:03:18\n",
      "\u001b[32m[05/11 22:19:02 d2.evaluation.evaluator]: \u001b[0mInference done 59/209. Dataloading: 0.0010 s/iter. Inference: 1.2819 s/iter. Eval: 0.0118 s/iter. Total: 1.2952 s/iter. ETA=0:03:14\n",
      "\u001b[32m[05/11 22:19:07 d2.evaluation.evaluator]: \u001b[0mInference done 63/209. Dataloading: 0.0010 s/iter. Inference: 1.2901 s/iter. Eval: 0.0115 s/iter. Total: 1.3030 s/iter. ETA=0:03:10\n",
      "\u001b[32m[05/11 22:19:12 d2.evaluation.evaluator]: \u001b[0mInference done 68/209. Dataloading: 0.0010 s/iter. Inference: 1.2675 s/iter. Eval: 0.0107 s/iter. Total: 1.2795 s/iter. ETA=0:03:00\n",
      "\u001b[32m[05/11 22:19:18 d2.evaluation.evaluator]: \u001b[0mInference done 73/209. Dataloading: 0.0009 s/iter. Inference: 1.2596 s/iter. Eval: 0.0112 s/iter. Total: 1.2722 s/iter. ETA=0:02:53\n",
      "\u001b[32m[05/11 22:19:24 d2.evaluation.evaluator]: \u001b[0mInference done 78/209. Dataloading: 0.0009 s/iter. Inference: 1.2522 s/iter. Eval: 0.0115 s/iter. Total: 1.2651 s/iter. ETA=0:02:45\n",
      "\u001b[32m[05/11 22:19:29 d2.evaluation.evaluator]: \u001b[0mInference done 82/209. Dataloading: 0.0009 s/iter. Inference: 1.2520 s/iter. Eval: 0.0111 s/iter. Total: 1.2645 s/iter. ETA=0:02:40\n",
      "\u001b[32m[05/11 22:19:35 d2.evaluation.evaluator]: \u001b[0mInference done 86/209. Dataloading: 0.0009 s/iter. Inference: 1.2592 s/iter. Eval: 0.0114 s/iter. Total: 1.2721 s/iter. ETA=0:02:36\n",
      "\u001b[32m[05/11 22:19:40 d2.evaluation.evaluator]: \u001b[0mInference done 90/209. Dataloading: 0.0009 s/iter. Inference: 1.2589 s/iter. Eval: 0.0110 s/iter. Total: 1.2712 s/iter. ETA=0:02:31\n",
      "\u001b[32m[05/11 22:19:45 d2.evaluation.evaluator]: \u001b[0mInference done 95/209. Dataloading: 0.0009 s/iter. Inference: 1.2482 s/iter. Eval: 0.0118 s/iter. Total: 1.2614 s/iter. ETA=0:02:23\n",
      "\u001b[32m[05/11 22:19:51 d2.evaluation.evaluator]: \u001b[0mInference done 99/209. Dataloading: 0.0009 s/iter. Inference: 1.2516 s/iter. Eval: 0.0132 s/iter. Total: 1.2663 s/iter. ETA=0:02:19\n",
      "\u001b[32m[05/11 22:19:57 d2.evaluation.evaluator]: \u001b[0mInference done 104/209. Dataloading: 0.0009 s/iter. Inference: 1.2459 s/iter. Eval: 0.0140 s/iter. Total: 1.2614 s/iter. ETA=0:02:12\n",
      "\u001b[32m[05/11 22:20:03 d2.evaluation.evaluator]: \u001b[0mInference done 109/209. Dataloading: 0.0009 s/iter. Inference: 1.2422 s/iter. Eval: 0.0136 s/iter. Total: 1.2572 s/iter. ETA=0:02:05\n",
      "\u001b[32m[05/11 22:20:10 d2.evaluation.evaluator]: \u001b[0mInference done 113/209. Dataloading: 0.0009 s/iter. Inference: 1.2595 s/iter. Eval: 0.0140 s/iter. Total: 1.2749 s/iter. ETA=0:02:02\n",
      "\u001b[32m[05/11 22:20:15 d2.evaluation.evaluator]: \u001b[0mInference done 116/209. Dataloading: 0.0009 s/iter. Inference: 1.2753 s/iter. Eval: 0.0139 s/iter. Total: 1.2906 s/iter. ETA=0:02:00\n",
      "\u001b[32m[05/11 22:20:21 d2.evaluation.evaluator]: \u001b[0mInference done 119/209. Dataloading: 0.0010 s/iter. Inference: 1.2917 s/iter. Eval: 0.0139 s/iter. Total: 1.3071 s/iter. ETA=0:01:57\n",
      "\u001b[32m[05/11 22:20:27 d2.evaluation.evaluator]: \u001b[0mInference done 122/209. Dataloading: 0.0010 s/iter. Inference: 1.3095 s/iter. Eval: 0.0138 s/iter. Total: 1.3249 s/iter. ETA=0:01:55\n",
      "\u001b[32m[05/11 22:20:32 d2.evaluation.evaluator]: \u001b[0mInference done 124/209. Dataloading: 0.0011 s/iter. Inference: 1.3289 s/iter. Eval: 0.0156 s/iter. Total: 1.3462 s/iter. ETA=0:01:54\n",
      "\u001b[32m[05/11 22:20:37 d2.evaluation.evaluator]: \u001b[0mInference done 128/209. Dataloading: 0.0011 s/iter. Inference: 1.3298 s/iter. Eval: 0.0151 s/iter. Total: 1.3466 s/iter. ETA=0:01:49\n",
      "\u001b[32m[05/11 22:20:43 d2.evaluation.evaluator]: \u001b[0mInference done 131/209. Dataloading: 0.0011 s/iter. Inference: 1.3436 s/iter. Eval: 0.0163 s/iter. Total: 1.3616 s/iter. ETA=0:01:46\n",
      "\u001b[32m[05/11 22:20:49 d2.evaluation.evaluator]: \u001b[0mInference done 134/209. Dataloading: 0.0011 s/iter. Inference: 1.3550 s/iter. Eval: 0.0182 s/iter. Total: 1.3749 s/iter. ETA=0:01:43\n",
      "\u001b[32m[05/11 22:20:55 d2.evaluation.evaluator]: \u001b[0mInference done 137/209. Dataloading: 0.0011 s/iter. Inference: 1.3669 s/iter. Eval: 0.0201 s/iter. Total: 1.3888 s/iter. ETA=0:01:39\n",
      "\u001b[32m[05/11 22:21:00 d2.evaluation.evaluator]: \u001b[0mInference done 140/209. Dataloading: 0.0011 s/iter. Inference: 1.3752 s/iter. Eval: 0.0200 s/iter. Total: 1.3969 s/iter. ETA=0:01:36\n",
      "\u001b[32m[05/11 22:21:06 d2.evaluation.evaluator]: \u001b[0mInference done 144/209. Dataloading: 0.0011 s/iter. Inference: 1.3728 s/iter. Eval: 0.0200 s/iter. Total: 1.3945 s/iter. ETA=0:01:30\n",
      "\u001b[32m[05/11 22:21:12 d2.evaluation.evaluator]: \u001b[0mInference done 148/209. Dataloading: 0.0011 s/iter. Inference: 1.3750 s/iter. Eval: 0.0198 s/iter. Total: 1.3966 s/iter. ETA=0:01:25\n",
      "\u001b[32m[05/11 22:21:17 d2.evaluation.evaluator]: \u001b[0mInference done 151/209. Dataloading: 0.0011 s/iter. Inference: 1.3812 s/iter. Eval: 0.0199 s/iter. Total: 1.4029 s/iter. ETA=0:01:21\n",
      "\u001b[32m[05/11 22:21:22 d2.evaluation.evaluator]: \u001b[0mInference done 154/209. Dataloading: 0.0011 s/iter. Inference: 1.3878 s/iter. Eval: 0.0196 s/iter. Total: 1.4092 s/iter. ETA=0:01:17\n",
      "\u001b[32m[05/11 22:21:27 d2.evaluation.evaluator]: \u001b[0mInference done 157/209. Dataloading: 0.0011 s/iter. Inference: 1.3969 s/iter. Eval: 0.0197 s/iter. Total: 1.4185 s/iter. ETA=0:01:13\n",
      "\u001b[32m[05/11 22:21:34 d2.evaluation.evaluator]: \u001b[0mInference done 161/209. Dataloading: 0.0011 s/iter. Inference: 1.4000 s/iter. Eval: 0.0194 s/iter. Total: 1.4212 s/iter. ETA=0:01:08\n",
      "\u001b[32m[05/11 22:21:39 d2.evaluation.evaluator]: \u001b[0mInference done 165/209. Dataloading: 0.0011 s/iter. Inference: 1.4006 s/iter. Eval: 0.0191 s/iter. Total: 1.4215 s/iter. ETA=0:01:02\n",
      "\u001b[32m[05/11 22:21:45 d2.evaluation.evaluator]: \u001b[0mInference done 169/209. Dataloading: 0.0011 s/iter. Inference: 1.3988 s/iter. Eval: 0.0189 s/iter. Total: 1.4195 s/iter. ETA=0:00:56\n",
      "\u001b[32m[05/11 22:21:50 d2.evaluation.evaluator]: \u001b[0mInference done 174/209. Dataloading: 0.0011 s/iter. Inference: 1.3897 s/iter. Eval: 0.0183 s/iter. Total: 1.4098 s/iter. ETA=0:00:49\n",
      "\u001b[32m[05/11 22:21:55 d2.evaluation.evaluator]: \u001b[0mInference done 178/209. Dataloading: 0.0011 s/iter. Inference: 1.3873 s/iter. Eval: 0.0179 s/iter. Total: 1.4070 s/iter. ETA=0:00:43\n",
      "\u001b[32m[05/11 22:22:01 d2.evaluation.evaluator]: \u001b[0mInference done 183/209. Dataloading: 0.0011 s/iter. Inference: 1.3806 s/iter. Eval: 0.0175 s/iter. Total: 1.3998 s/iter. ETA=0:00:36\n",
      "\u001b[32m[05/11 22:22:06 d2.evaluation.evaluator]: \u001b[0mInference done 188/209. Dataloading: 0.0011 s/iter. Inference: 1.3726 s/iter. Eval: 0.0170 s/iter. Total: 1.3913 s/iter. ETA=0:00:29\n",
      "\u001b[32m[05/11 22:22:12 d2.evaluation.evaluator]: \u001b[0mInference done 192/209. Dataloading: 0.0011 s/iter. Inference: 1.3733 s/iter. Eval: 0.0186 s/iter. Total: 1.3936 s/iter. ETA=0:00:23\n",
      "\u001b[32m[05/11 22:22:17 d2.evaluation.evaluator]: \u001b[0mInference done 197/209. Dataloading: 0.0011 s/iter. Inference: 1.3639 s/iter. Eval: 0.0181 s/iter. Total: 1.3837 s/iter. ETA=0:00:16\n",
      "\u001b[32m[05/11 22:22:23 d2.evaluation.evaluator]: \u001b[0mInference done 201/209. Dataloading: 0.0010 s/iter. Inference: 1.3616 s/iter. Eval: 0.0180 s/iter. Total: 1.3813 s/iter. ETA=0:00:11\n",
      "\u001b[32m[05/11 22:22:29 d2.evaluation.evaluator]: \u001b[0mInference done 205/209. Dataloading: 0.0011 s/iter. Inference: 1.3642 s/iter. Eval: 0.0183 s/iter. Total: 1.3842 s/iter. ETA=0:00:05\n",
      "\u001b[32m[05/11 22:22:34 d2.evaluation.evaluator]: \u001b[0mInference done 209/209. Dataloading: 0.0011 s/iter. Inference: 1.3631 s/iter. Eval: 0.0181 s/iter. Total: 1.3829 s/iter. ETA=0:00:00\n",
      "\u001b[32m[05/11 22:22:34 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:04:42.594436 (1.385267 s / iter per device, on 1 devices)\n",
      "\u001b[32m[05/11 22:22:34 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:04:38 (1.363135 s / iter per device, on 1 devices)\n",
      "\u001b[32m[05/11 22:22:34 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[05/11 22:22:34 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./cell_segmentation_output/coco_instances_results.json\n",
      "\u001b[32m[05/11 22:22:34 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[05/11 22:22:34 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[05/11 22:22:35 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.08 seconds.\n",
      "\u001b[32m[05/11 22:22:35 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[05/11 22:22:35 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.00 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.723\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.953\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.854\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.191\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.707\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.781\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.066\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.428\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.771\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.323\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.748\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.842\n",
      "\u001b[32m[05/11 22:22:35 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 72.295 | 95.251 | 85.370 | 19.135 | 70.726 | 78.150 |\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[05/11 22:22:35 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *segm*\n",
      "\u001b[32m[05/11 22:22:35 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.09 seconds.\n",
      "\u001b[32m[05/11 22:22:35 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[05/11 22:22:35 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.00 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.719\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.952\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.857\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.089\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.690\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.804\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.066\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.426\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.761\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.266\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.735\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.841\n",
      "\u001b[32m[05/11 22:22:35 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
      "| 71.880 | 95.189 | 85.733 | 8.922 | 69.009 | 80.390 |\n",
      "\u001b[32m[05/11 22:22:35 d2.engine.defaults]: \u001b[0mEvaluation results for cell_val in csv format:\n",
      "\u001b[32m[05/11 22:22:35 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
      "\u001b[32m[05/11 22:22:35 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[05/11 22:22:35 d2.evaluation.testing]: \u001b[0mcopypaste: 72.2948,95.2512,85.3697,19.1345,70.7257,78.1496\n",
      "\u001b[32m[05/11 22:22:35 d2.evaluation.testing]: \u001b[0mcopypaste: Task: segm\n",
      "\u001b[32m[05/11 22:22:35 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[05/11 22:22:35 d2.evaluation.testing]: \u001b[0mcopypaste: 71.8797,95.1892,85.7334,8.9224,69.0088,80.3901\n",
      "\u001b[32m[05/11 22:22:35 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from ./cell_segmentation_output/model_final.pth ...\n",
      "\u001b[32m[05/11 22:24:03 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[05/11 22:24:03 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[05/11 22:24:03 d2.data.common]: \u001b[0mSerializing 209 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[05/11 22:24:03 d2.data.common]: \u001b[0mSerialized dataset takes 2.03 MiB\n",
      "\u001b[32m[05/11 22:24:03 d2.evaluation.evaluator]: \u001b[0mStart inference on 209 batches\n",
      "\u001b[32m[05/11 22:24:17 d2.evaluation.evaluator]: \u001b[0mInference done 11/209. Dataloading: 0.0003 s/iter. Inference: 1.0722 s/iter. Eval: 0.0072 s/iter. Total: 1.0797 s/iter. ETA=0:03:33\n",
      "\u001b[32m[05/11 22:24:23 d2.evaluation.evaluator]: \u001b[0mInference done 16/209. Dataloading: 0.0004 s/iter. Inference: 1.0650 s/iter. Eval: 0.0068 s/iter. Total: 1.0725 s/iter. ETA=0:03:26\n",
      "\u001b[32m[05/11 22:24:28 d2.evaluation.evaluator]: \u001b[0mInference done 21/209. Dataloading: 0.0006 s/iter. Inference: 1.0700 s/iter. Eval: 0.0081 s/iter. Total: 1.0790 s/iter. ETA=0:03:22\n",
      "\u001b[32m[05/11 22:24:34 d2.evaluation.evaluator]: \u001b[0mInference done 27/209. Dataloading: 0.0005 s/iter. Inference: 1.0533 s/iter. Eval: 0.0091 s/iter. Total: 1.0635 s/iter. ETA=0:03:13\n",
      "\u001b[32m[05/11 22:24:39 d2.evaluation.evaluator]: \u001b[0mInference done 31/209. Dataloading: 0.0005 s/iter. Inference: 1.0934 s/iter. Eval: 0.0089 s/iter. Total: 1.1034 s/iter. ETA=0:03:16\n",
      "\u001b[32m[05/11 22:24:45 d2.evaluation.evaluator]: \u001b[0mInference done 37/209. Dataloading: 0.0005 s/iter. Inference: 1.0659 s/iter. Eval: 0.0079 s/iter. Total: 1.0748 s/iter. ETA=0:03:04\n",
      "\u001b[32m[05/11 22:24:51 d2.evaluation.evaluator]: \u001b[0mInference done 43/209. Dataloading: 0.0005 s/iter. Inference: 1.0462 s/iter. Eval: 0.0071 s/iter. Total: 1.0543 s/iter. ETA=0:02:55\n",
      "\u001b[32m[05/11 22:24:56 d2.evaluation.evaluator]: \u001b[0mInference done 48/209. Dataloading: 0.0005 s/iter. Inference: 1.0397 s/iter. Eval: 0.0080 s/iter. Total: 1.0485 s/iter. ETA=0:02:48\n",
      "\u001b[32m[05/11 22:25:01 d2.evaluation.evaluator]: \u001b[0mInference done 53/209. Dataloading: 0.0005 s/iter. Inference: 1.0462 s/iter. Eval: 0.0079 s/iter. Total: 1.0550 s/iter. ETA=0:02:44\n",
      "\u001b[32m[05/11 22:25:07 d2.evaluation.evaluator]: \u001b[0mInference done 58/209. Dataloading: 0.0005 s/iter. Inference: 1.0504 s/iter. Eval: 0.0079 s/iter. Total: 1.0591 s/iter. ETA=0:02:39\n",
      "\u001b[32m[05/11 22:25:12 d2.evaluation.evaluator]: \u001b[0mInference done 63/209. Dataloading: 0.0005 s/iter. Inference: 1.0464 s/iter. Eval: 0.0079 s/iter. Total: 1.0551 s/iter. ETA=0:02:34\n",
      "\u001b[32m[05/11 22:25:18 d2.evaluation.evaluator]: \u001b[0mInference done 69/209. Dataloading: 0.0005 s/iter. Inference: 1.0412 s/iter. Eval: 0.0076 s/iter. Total: 1.0497 s/iter. ETA=0:02:26\n",
      "\u001b[32m[05/11 22:25:24 d2.evaluation.evaluator]: \u001b[0mInference done 74/209. Dataloading: 0.0005 s/iter. Inference: 1.0491 s/iter. Eval: 0.0082 s/iter. Total: 1.0581 s/iter. ETA=0:02:22\n",
      "\u001b[32m[05/11 22:25:29 d2.evaluation.evaluator]: \u001b[0mInference done 79/209. Dataloading: 0.0005 s/iter. Inference: 1.0462 s/iter. Eval: 0.0082 s/iter. Total: 1.0552 s/iter. ETA=0:02:17\n",
      "\u001b[32m[05/11 22:25:34 d2.evaluation.evaluator]: \u001b[0mInference done 84/209. Dataloading: 0.0005 s/iter. Inference: 1.0476 s/iter. Eval: 0.0082 s/iter. Total: 1.0566 s/iter. ETA=0:02:12\n",
      "\u001b[32m[05/11 22:25:40 d2.evaluation.evaluator]: \u001b[0mInference done 89/209. Dataloading: 0.0005 s/iter. Inference: 1.0518 s/iter. Eval: 0.0081 s/iter. Total: 1.0607 s/iter. ETA=0:02:07\n",
      "\u001b[32m[05/11 22:25:45 d2.evaluation.evaluator]: \u001b[0mInference done 94/209. Dataloading: 0.0005 s/iter. Inference: 1.0535 s/iter. Eval: 0.0087 s/iter. Total: 1.0629 s/iter. ETA=0:02:02\n",
      "\u001b[32m[05/11 22:25:51 d2.evaluation.evaluator]: \u001b[0mInference done 99/209. Dataloading: 0.0005 s/iter. Inference: 1.0575 s/iter. Eval: 0.0095 s/iter. Total: 1.0678 s/iter. ETA=0:01:57\n",
      "\u001b[32m[05/11 22:25:57 d2.evaluation.evaluator]: \u001b[0mInference done 104/209. Dataloading: 0.0005 s/iter. Inference: 1.0579 s/iter. Eval: 0.0101 s/iter. Total: 1.0687 s/iter. ETA=0:01:52\n",
      "\u001b[32m[05/11 22:26:02 d2.evaluation.evaluator]: \u001b[0mInference done 109/209. Dataloading: 0.0005 s/iter. Inference: 1.0566 s/iter. Eval: 0.0098 s/iter. Total: 1.0672 s/iter. ETA=0:01:46\n",
      "\u001b[32m[05/11 22:26:08 d2.evaluation.evaluator]: \u001b[0mInference done 114/209. Dataloading: 0.0005 s/iter. Inference: 1.0619 s/iter. Eval: 0.0100 s/iter. Total: 1.0726 s/iter. ETA=0:01:41\n",
      "\u001b[32m[05/11 22:26:13 d2.evaluation.evaluator]: \u001b[0mInference done 119/209. Dataloading: 0.0004 s/iter. Inference: 1.0597 s/iter. Eval: 0.0098 s/iter. Total: 1.0702 s/iter. ETA=0:01:36\n",
      "\u001b[32m[05/11 22:26:19 d2.evaluation.evaluator]: \u001b[0mInference done 124/209. Dataloading: 0.0004 s/iter. Inference: 1.0623 s/iter. Eval: 0.0108 s/iter. Total: 1.0738 s/iter. ETA=0:01:31\n",
      "\u001b[32m[05/11 22:26:25 d2.evaluation.evaluator]: \u001b[0mInference done 130/209. Dataloading: 0.0004 s/iter. Inference: 1.0596 s/iter. Eval: 0.0106 s/iter. Total: 1.0709 s/iter. ETA=0:01:24\n",
      "\u001b[32m[05/11 22:26:31 d2.evaluation.evaluator]: \u001b[0mInference done 135/209. Dataloading: 0.0004 s/iter. Inference: 1.0649 s/iter. Eval: 0.0124 s/iter. Total: 1.0780 s/iter. ETA=0:01:19\n",
      "\u001b[32m[05/11 22:26:37 d2.evaluation.evaluator]: \u001b[0mInference done 140/209. Dataloading: 0.0004 s/iter. Inference: 1.0664 s/iter. Eval: 0.0128 s/iter. Total: 1.0799 s/iter. ETA=0:01:14\n",
      "\u001b[32m[05/11 22:26:43 d2.evaluation.evaluator]: \u001b[0mInference done 146/209. Dataloading: 0.0004 s/iter. Inference: 1.0630 s/iter. Eval: 0.0126 s/iter. Total: 1.0763 s/iter. ETA=0:01:07\n",
      "\u001b[32m[05/11 22:26:48 d2.evaluation.evaluator]: \u001b[0mInference done 151/209. Dataloading: 0.0004 s/iter. Inference: 1.0635 s/iter. Eval: 0.0126 s/iter. Total: 1.0768 s/iter. ETA=0:01:02\n",
      "\u001b[32m[05/11 22:26:54 d2.evaluation.evaluator]: \u001b[0mInference done 156/209. Dataloading: 0.0004 s/iter. Inference: 1.0660 s/iter. Eval: 0.0125 s/iter. Total: 1.0792 s/iter. ETA=0:00:57\n",
      "\u001b[32m[05/11 22:26:59 d2.evaluation.evaluator]: \u001b[0mInference done 161/209. Dataloading: 0.0004 s/iter. Inference: 1.0687 s/iter. Eval: 0.0123 s/iter. Total: 1.0816 s/iter. ETA=0:00:51\n",
      "\u001b[32m[05/11 22:27:05 d2.evaluation.evaluator]: \u001b[0mInference done 166/209. Dataloading: 0.0004 s/iter. Inference: 1.0672 s/iter. Eval: 0.0120 s/iter. Total: 1.0799 s/iter. ETA=0:00:46\n",
      "\u001b[32m[05/11 22:27:10 d2.evaluation.evaluator]: \u001b[0mInference done 172/209. Dataloading: 0.0004 s/iter. Inference: 1.0637 s/iter. Eval: 0.0117 s/iter. Total: 1.0761 s/iter. ETA=0:00:39\n",
      "\u001b[32m[05/11 22:27:16 d2.evaluation.evaluator]: \u001b[0mInference done 178/209. Dataloading: 0.0004 s/iter. Inference: 1.0564 s/iter. Eval: 0.0113 s/iter. Total: 1.0684 s/iter. ETA=0:00:33\n",
      "\u001b[32m[05/11 22:27:21 d2.evaluation.evaluator]: \u001b[0mInference done 184/209. Dataloading: 0.0004 s/iter. Inference: 1.0536 s/iter. Eval: 0.0110 s/iter. Total: 1.0653 s/iter. ETA=0:00:26\n",
      "\u001b[32m[05/11 22:27:28 d2.evaluation.evaluator]: \u001b[0mInference done 190/209. Dataloading: 0.0004 s/iter. Inference: 1.0537 s/iter. Eval: 0.0113 s/iter. Total: 1.0657 s/iter. ETA=0:00:20\n",
      "\u001b[32m[05/11 22:27:34 d2.evaluation.evaluator]: \u001b[0mInference done 196/209. Dataloading: 0.0004 s/iter. Inference: 1.0501 s/iter. Eval: 0.0116 s/iter. Total: 1.0623 s/iter. ETA=0:00:13\n",
      "\u001b[32m[05/11 22:27:39 d2.evaluation.evaluator]: \u001b[0mInference done 201/209. Dataloading: 0.0004 s/iter. Inference: 1.0495 s/iter. Eval: 0.0114 s/iter. Total: 1.0616 s/iter. ETA=0:00:08\n",
      "\u001b[32m[05/11 22:27:45 d2.evaluation.evaluator]: \u001b[0mInference done 206/209. Dataloading: 0.0004 s/iter. Inference: 1.0530 s/iter. Eval: 0.0117 s/iter. Total: 1.0653 s/iter. ETA=0:00:03\n",
      "\u001b[32m[05/11 22:27:48 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:03:37.607184 (1.066702 s / iter per device, on 1 devices)\n",
      "\u001b[32m[05/11 22:27:48 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:03:34 (1.052285 s / iter per device, on 1 devices)\n",
      "\u001b[32m[05/11 22:27:48 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[05/11 22:27:48 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./cell_segmentation_output/coco_instances_results.json\n",
      "\u001b[32m[05/11 22:27:48 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[05/11 22:27:48 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[05/11 22:27:48 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.03 seconds.\n",
      "\u001b[32m[05/11 22:27:48 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[05/11 22:27:48 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.00 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.696\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.906\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.828\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.109\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.673\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.777\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.066\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.426\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.740\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.131\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.709\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.837\n",
      "\u001b[32m[05/11 22:27:48 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 69.633 | 90.642 | 82.813 | 10.908 | 67.328 | 77.693 |\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[05/11 22:27:48 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *segm*\n",
      "\u001b[32m[05/11 22:27:49 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.08 seconds.\n",
      "\u001b[32m[05/11 22:27:49 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[05/11 22:27:49 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.00 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.693\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.906\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.832\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.032\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.659\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.798\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.066\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.424\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.732\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.066\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.699\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.835\n",
      "\u001b[32m[05/11 22:27:49 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
      "| 69.335 | 90.586 | 83.204 | 3.163 | 65.912 | 79.782 |\n",
      "Validation Results: OrderedDict([('bbox', {'AP': 69.63331400196738, 'AP50': 90.64209356091524, 'AP75': 82.81338083929288, 'APs': 10.908415841584159, 'APm': 67.32779386299809, 'APl': 77.69251875855247}), ('segm', {'AP': 69.33514539139219, 'AP50': 90.58604153483823, 'AP75': 83.20358769434148, 'APs': 3.1633663366336635, 'APm': 65.91231821643258, 'APl': 79.78210711781153})])\n"
     ]
    }
   ],
   "source": [
    "# Optional: Uncomment to train\n",
    "trained_cfg = train_instance_segmentation(dataset_directory2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on New Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using trained_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_new_images(cfg, image_paths):\n",
    "    \"\"\"\n",
    "    Run inference on new images\n",
    "    \n",
    "    Args:\n",
    "        cfg: Detectron2 config\n",
    "        image_paths: List of paths to images\n",
    "    \"\"\"\n",
    "    # Load the trained model\n",
    "    cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # Confidence threshold\n",
    "    predictor = DefaultPredictor(cfg)\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = os.path.join(cfg.OUTPUT_DIR, \"new_predictions\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Process each image\n",
    "    import cv2\n",
    "    from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "    \n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        # Read image\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            print(f\"Could not read image: {image_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Run inference\n",
    "        outputs = predictor(img)\n",
    "        instances = outputs[\"instances\"].to(\"cpu\")\n",
    "        \n",
    "        # Visualize results\n",
    "        v = Visualizer(\n",
    "            img[:, :, ::-1],\n",
    "            scale=1.0,\n",
    "            instance_mode=ColorMode.SEGMENTATION\n",
    "        )\n",
    "        v = v.draw_instance_predictions(instances)\n",
    "        result_img = v.get_image()[:, :, ::-1]\n",
    "        \n",
    "        # Save results\n",
    "        output_path = os.path.join(output_dir, f\"pred_{os.path.basename(image_path)}\")\n",
    "        cv2.imwrite(output_path, result_img)\n",
    "        \n",
    "        print(f\"Processed {image_path}: Found {len(instances)} cell instances\")\n",
    "        \n",
    "        # Extract mask data if needed\n",
    "        masks = instances.pred_masks.numpy() if instances.has(\"pred_masks\") else None\n",
    "        scores = instances.scores.numpy() if instances.has(\"scores\") else None\n",
    "        \n",
    "        # You can save masks or do further analysis here\n",
    "    \n",
    "    print(f\"All predictions saved to {output_dir}\")\n",
    "\n",
    "# Example usage\n",
    "# image_paths = ['/Users/niti/Desktop/macOS_D/sem8/CP303/Compilation-Data-170325/FISH-All-Consolidated-Data/Cell-Only/AL 224_23.tif']\n",
    "# predict_on_new_images(trained_cfg, image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[05/12 02:04:44 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from ./cell_segmentation_output/model_final.pth ...\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/AL 276_24_KMT2A.png: Found 19 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MBN 80_24_CCND1_BA_XL.png: Found 6 cell instances\n",
      "All predictions saved to ./cell_segmentation_output/new_predictions\n"
     ]
    }
   ],
   "source": [
    "image_paths = ['/Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/AL 276_24_KMT2A.png', '/Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MBN 80_24_CCND1_BA_XL.png']\n",
    "predict_on_new_images(trained_cfg, image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Saved Model + Save Instance Masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[05/12 09:55:52 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from ./cell_segmentation_output/model_final.pth ...\n"
     ]
    }
   ],
   "source": [
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor\n",
    "import os\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "def load_cell_segmentation_model(model_path):\n",
    "    \"\"\"\n",
    "    Load a trained cell segmentation model\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to model_final.pth file\n",
    "    \n",
    "    Returns:\n",
    "        cfg: Configuration\n",
    "        predictor: Loaded model predictor\n",
    "    \"\"\"\n",
    "    # Get a base config\n",
    "    cfg = get_cfg()\n",
    "    \n",
    "    # Add your model's configuration\n",
    "    # This part needs to match how you configured the training\n",
    "    cfg_path = \"/opt/anaconda3/envs/detectron2-env/lib/python3.9/site-packages/detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"\n",
    "    cfg.merge_from_file(cfg_path)\n",
    "    cfg.MODEL.DEVICE = \"cpu\"\n",
    "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # Set to match your original training\n",
    "    \n",
    "    # Set model path and inference threshold\n",
    "    cfg.MODEL.WEIGHTS = model_path\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7\n",
    "    \n",
    "    # Create predictor\n",
    "    predictor = DefaultPredictor(cfg)\n",
    "    \n",
    "    return cfg, predictor\n",
    "\n",
    "def prediction_maker(predictor, image_paths):\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = os.path.join(cfg.OUTPUT_DIR, \"new_predictions\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    combined_masks_dir = os.path.join(cfg.OUTPUT_DIR, \"instance_masks_combined\")\n",
    "    os.makedirs(combined_masks_dir, exist_ok=True)\n",
    "    masks_dir = os.path.join(cfg.OUTPUT_DIR, \"instance_masks\")\n",
    "    os.makedirs(masks_dir, exist_ok=True)\n",
    "\n",
    "    # Process each image\n",
    "    import cv2\n",
    "    from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "    \n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        # Read image\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            print(f\"Could not read image: {image_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Run inference\n",
    "        outputs = predictor(img)\n",
    "        instances = outputs[\"instances\"].to(\"cpu\")\n",
    "        \n",
    "        # Visualize results\n",
    "        v = Visualizer(\n",
    "            img[:, :, ::-1],\n",
    "            scale=1.0,\n",
    "            instance_mode=ColorMode.SEGMENTATION\n",
    "        )\n",
    "        v = v.draw_instance_predictions(instances)\n",
    "        result_img = v.get_image()[:, :, ::-1]\n",
    "        \n",
    "        # Save results\n",
    "        output_path = os.path.join(output_dir, f\"pred_{os.path.basename(image_path)}\")\n",
    "        cv2.imwrite(output_path, result_img)\n",
    "        \n",
    "        print(f\"Processed {image_path}: Found {len(instances)} cell instances\")\n",
    "        \n",
    "        # Extract mask data if needed\n",
    "        masks = instances.pred_masks.numpy() if instances.has(\"pred_masks\") else None\n",
    "        scores = instances.scores.numpy() if instances.has(\"scores\") else None\n",
    "\n",
    "        if masks is not None:\n",
    "            base_filename = os.path.splitext(os.path.basename(image_path))[0]\n",
    "            num_instances = masks.shape[0]\n",
    "\n",
    "            # Combined Mask Image\n",
    "            if num_instances > 0:\n",
    "                # Create a color-coded mask image where each instance has a different color\n",
    "                height, width = masks[0].shape\n",
    "                combined_mask = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "                \n",
    "                # Generate unique colors for each instance\n",
    "                colors = [\n",
    "                    (np.random.randint(0, 255), np.random.randint(0, 255), np.random.randint(0, 255))\n",
    "                    for _ in range(num_instances)\n",
    "                ]\n",
    "                \n",
    "                # Add each mask with its unique color\n",
    "                for j, mask in enumerate(masks):\n",
    "                    color_mask = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "                    color_mask[mask] = colors[j]\n",
    "                    combined_mask = cv.bitwise_or(combined_mask, color_mask)\n",
    "                \n",
    "                # Save combined mask\n",
    "                combined_mask_path = os.path.join(combined_masks_dir, f\"{base_filename}_all_instances.png\")\n",
    "                cv2.imwrite(combined_mask_path, combined_mask)\n",
    "            \n",
    "            # Single Masks Saved\n",
    "            if num_instances > 0:\n",
    "                current_masks_dir = os.path.join(masks_dir, base_filename)\n",
    "                os.makedirs(current_masks_dir, exist_ok=True)\n",
    "\n",
    "                for j, mask in enumerate(masks):\n",
    "                    # Create a blank grayscale image\n",
    "                    single_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "                    \n",
    "                    # Set mask area to white (255)\n",
    "                    single_mask[mask] = 255\n",
    "                    \n",
    "                    # Save the single instance mask\n",
    "                    single_mask_path = os.path.join(current_masks_dir, f\"{base_filename}_instance_{j+1}.png\")\n",
    "                    cv2.imwrite(single_mask_path, single_mask)\n",
    "                    \n",
    "        # You can save masks or do further analysis here\n",
    "    \n",
    "    print(f\"All predictions saved to {output_dir}\")\n",
    "# Usage\n",
    "model_path = \"./cell_segmentation_output/model_final.pth\"\n",
    "cfg, predictor = load_cell_segmentation_model(model_path)\n",
    "# image_paths = ['/Users/niti/Desktop/macOS_D/sem8/CP303/Compilation-Data-170325/FISH-All-Consolidated-Data/Cell-Only/AL 224_23.tif']\n",
    "# prediction_maker(predictor, image_paths)\n",
    "# Now you can use predictor for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Predictions on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MBN 108_24_CEN11ATM_XL.png: Found 14 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MM 195_24_5P9Q15Q_26_06_2024.png: Found 2 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MBN 80_24_CCND1_BA_XL.png: Found 6 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MDS 601_23_8.png: Found 66 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MM 127_24_PLOIDY_06_05_2024.png: Found 4 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MDS 254_24_CEN8.png: Found 2 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MM 92_24_1Q1P_XL.png: Found 36 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MM 159_24_TP53_NF1.png: Found 5 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/AL 152_24_TCF3_PBX1_XL_1.png: Found 25 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MDS 601_23_5Q_05_01_2024.png: Found 46 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MM 171_24_FGFR3_IGH.png: Found 25 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MDS 581_23_CEN8.png: Found 15 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MDS 106_24_MECOM3_XL.png: Found 12 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/AL 200_24_PLOIDY_XL_02_05_2024.png: Found 3 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MM 142_24_MYC_BA_XL.png: Found 6 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/AL 523_22_CCND1_BA.png: Found 38 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/AL 291_24_PDGFRB.png: Found 1 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MM 125_24_CCND1_BA_XL.png: Found 5 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/Al 180_24_RARA_BA_2.png: Found 6 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MM 94_24_FGFR3_XL_1.png: Found 15 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/AL 123_24_CEN8.png: Found 10 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MDS 589_23_7Q_05_01_2024.png: Found 40 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MM 83_24_1Q1P_XL.png: Found 20 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MDS 1_24_8_18_01_2024.png: Found 77 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MDS 152_24_CEN_7_XL_02_05_2024.png: Found 12 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MM 102_24_MAFIGH_XL.png: Found 3 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/FH 133_24_MYC.png: Found 4 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MM 50_24_1Q1P_XL_1.png: Found 3 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/AL 111_24_BCR_ABL_1_XL_2.png: Found 2 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/AL 276_24_KMT2A.png: Found 19 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MBN 92_24_ATMCEN11_XL.png: Found 22 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MDS 160_24_DEL_20_XL_1.png: Found 8 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MM 130_24_1P_1Q_XL.png: Found 13 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MDS 128_24_BCRABL1_XL.png: Found 20 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MM 92_24_FGFR3_XL.png: Found 16 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MM 170_24_1P1Q.png: Found 8 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/AL 102_24_7Q_CEN17_XL_02_05_2024.png: Found 11 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MM 97_24_1P1Q_XL.png: Found 14 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/AL 157_24_TCF3_PBX1_XL.png: Found 6 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MM 158_24_MYC.png: Found 2 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MM 209_24_CCND1.png: Found 4 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MBN 155_24_TP53_CEN17.png: Found 2 cell instances\n",
      "All predictions saved to ./output/new_predictions\n"
     ]
    }
   ],
   "source": [
    "image_paths = []\n",
    "test_images_dir = \"/Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images\"\n",
    "for filename in os.listdir(test_images_dir):\n",
    "    if filename.endswith(('.png', '.jpg', '.tif')):\n",
    "        image_paths.append(os.path.join(test_images_dir, filename))\n",
    "prediction_maker(predictor, image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on test data from: /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset\n",
      "\u001b[32m[05/12 02:05:48 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from ./cell_segmentation_output/model_final.pth ...\n",
      "\u001b[32m[05/12 02:05:48 d2.evaluation.coco_evaluation]: \u001b[0mTrying to convert 'cell_test' to COCO format ...\n",
      "\u001b[32m[05/12 02:05:48 d2.data.datasets.coco]: \u001b[0mConverting annotations of dataset 'cell_test' to COCO format ...)\n",
      "\u001b[32m[05/12 02:06:10 d2.data.datasets.coco]: \u001b[0mConverting dataset dicts into COCO format\n",
      "\u001b[32m[05/12 02:06:10 d2.data.datasets.coco]: \u001b[0mConversion finished, #images: 42, #annotations: 692\n",
      "\u001b[32m[05/12 02:06:10 d2.data.datasets.coco]: \u001b[0mCaching COCO format annotations at './cell_segmentation_output/test_evaluation/cell_test_coco_format.json' ...\n",
      "\u001b[32m[05/12 02:06:33 d2.data.build]: \u001b[0mDistribution of instances among all 1 categories:\n",
      "\u001b[36m|  category  | #instances   |\n",
      "|:----------:|:-------------|\n",
      "|    cell    | 692          |\n",
      "|            |              |\u001b[0m\n",
      "\u001b[32m[05/12 02:06:33 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[05/12 02:06:33 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[05/12 02:06:33 d2.data.common]: \u001b[0mSerializing 42 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[05/12 02:06:33 d2.data.common]: \u001b[0mSerialized dataset takes 0.50 MiB\n",
      "Running evaluation on cell_test...\n",
      "\u001b[32m[05/12 02:06:33 d2.evaluation.evaluator]: \u001b[0mStart inference on 42 batches\n",
      "\u001b[32m[05/12 02:06:47 d2.evaluation.evaluator]: \u001b[0mInference done 11/42. Dataloading: 0.0003 s/iter. Inference: 1.0041 s/iter. Eval: 0.0066 s/iter. Total: 1.0109 s/iter. ETA=0:00:31\n",
      "\u001b[32m[05/12 02:06:52 d2.evaluation.evaluator]: \u001b[0mInference done 16/42. Dataloading: 0.0003 s/iter. Inference: 1.0293 s/iter. Eval: 0.0080 s/iter. Total: 1.0377 s/iter. ETA=0:00:26\n",
      "\u001b[32m[05/12 02:06:57 d2.evaluation.evaluator]: \u001b[0mInference done 21/42. Dataloading: 0.0003 s/iter. Inference: 1.0430 s/iter. Eval: 0.0143 s/iter. Total: 1.0579 s/iter. ETA=0:00:22\n",
      "\u001b[32m[05/12 02:07:03 d2.evaluation.evaluator]: \u001b[0mInference done 26/42. Dataloading: 0.0003 s/iter. Inference: 1.0450 s/iter. Eval: 0.0198 s/iter. Total: 1.0653 s/iter. ETA=0:00:17\n",
      "\u001b[32m[05/12 02:07:08 d2.evaluation.evaluator]: \u001b[0mInference done 31/42. Dataloading: 0.0004 s/iter. Inference: 1.0462 s/iter. Eval: 0.0163 s/iter. Total: 1.0631 s/iter. ETA=0:00:11\n",
      "\u001b[32m[05/12 02:07:14 d2.evaluation.evaluator]: \u001b[0mInference done 36/42. Dataloading: 0.0004 s/iter. Inference: 1.0543 s/iter. Eval: 0.0144 s/iter. Total: 1.0694 s/iter. ETA=0:00:06\n",
      "\u001b[32m[05/12 02:07:20 d2.evaluation.evaluator]: \u001b[0mInference done 41/42. Dataloading: 0.0004 s/iter. Inference: 1.0802 s/iter. Eval: 0.0154 s/iter. Total: 1.0962 s/iter. ETA=0:00:01\n",
      "\u001b[32m[05/12 02:07:22 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:41.099375 (1.110794 s / iter per device, on 1 devices)\n",
      "\u001b[32m[05/12 02:07:22 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:40 (1.082423 s / iter per device, on 1 devices)\n",
      "\u001b[32m[05/12 02:07:22 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[05/12 02:07:22 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./cell_segmentation_output/test_evaluation/coco_instances_results.json\n",
      "\u001b[32m[05/12 02:07:22 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[05/12 02:07:22 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[05/12 02:07:22 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.02 seconds.\n",
      "\u001b[32m[05/12 02:07:22 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[05/12 02:07:22 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.00 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.701\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.933\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.836\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.154\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.692\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.767\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.055\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.371\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.747\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.171\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.727\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.831\n",
      "\u001b[32m[05/12 02:07:22 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 70.085 | 93.347 | 83.561 | 15.446 | 69.183 | 76.698 |\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[05/12 02:07:22 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *segm*\n",
      "\u001b[32m[05/12 02:07:22 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.03 seconds.\n",
      "\u001b[32m[05/12 02:07:22 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[05/12 02:07:22 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.00 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.698\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.930\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.838\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.673\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.799\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.054\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.368\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.738\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.029\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.717\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.831\n",
      "\u001b[32m[05/12 02:07:22 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
      "| 69.810 | 93.038 | 83.818 | 0.990 | 67.298 | 79.906 |\n",
      "\n",
      "=== Evaluation Results ===\n",
      "\n",
      "bbox metrics:\n",
      "  AP: 70.08549170245911\n",
      "  AP50: 93.34691498853827\n",
      "  AP75: 83.56060500702986\n",
      "  APs: 15.445544554455445\n",
      "  APm: 69.18336169943407\n",
      "  APl: 76.6976276323211\n",
      "\n",
      "segm metrics:\n",
      "  AP: 69.81018054536273\n",
      "  AP50: 93.03765166963463\n",
      "  AP75: 83.81809851674815\n",
      "  APs: 0.9900990099009901\n",
      "  APm: 67.29790410848953\n",
      "  APl: 79.90589242295145\n",
      "\u001b[32m[05/12 02:07:22 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from ./cell_segmentation_output/model_final.pth ...\n",
      "Processed image 1/5: MM 94_24_FGFR3_XL_1.png\n",
      "  Found 15 predictions\n",
      "  Ground truth: 16 annotations\n",
      "Processed image 2/5: AL 291_24_PDGFRB.png\n",
      "  Found 2 predictions\n",
      "  Ground truth: 1 annotations\n",
      "Processed image 3/5: AL 111_24_BCR_ABL_1_XL_2.png\n",
      "  Found 2 predictions\n",
      "  Ground truth: 2 annotations\n",
      "Processed image 4/5: MDS 152_24_CEN_7_XL_02_05_2024.png\n",
      "  Found 13 predictions\n",
      "  Ground truth: 12 annotations\n",
      "Processed image 5/5: MDS 106_24_MECOM3_XL.png\n",
      "  Found 12 predictions\n",
      "  Ground truth: 12 annotations\n",
      "\n",
      "Visualizations saved to: ./cell_segmentation_output/test_evaluation/visualizations\n",
      "\n",
      "Full evaluation results saved to: ./cell_segmentation_output/test_evaluation\n",
      "\n",
      "=== Additional Analysis ===\n",
      "\n",
      "Segmentation Metrics:\n",
      "  Mean AP: 69.810\n",
      "  AP by size: small=0.000, medium=0.000, large=0.000\n",
      "  AP at IoU: AP50=93.038, AP75=83.818\n",
      "\n",
      "  ✅ Model performance is good for cell segmentation tasks\n",
      "\n",
      "Bounding Box Detection Metrics:\n",
      "  Mean AP: 70.085\n",
      "  AP50: 93.347\n",
      "\u001b[32m[05/12 02:07:51 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from ./cell_segmentation_output/model_final.pth ...\n",
      "\n",
      "=== Per-Image Summary ===\n",
      "Total images: 42\n",
      "Total ground truth instances: 692\n",
      "Total predicted instances: 701\n",
      "Average instances per image: 16.48\n",
      "Average predictions per image: 16.69\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.engine import DefaultTrainer, DefaultPredictor\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import build_detection_test_loader\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "import random\n",
    "\n",
    "def evaluate_on_test_data(trained_cfg, test_data_dir):\n",
    "    \"\"\"\n",
    "    Evaluate trained model on test data\n",
    "    \n",
    "    Args:\n",
    "        trained_cfg: Trained detectron2 config\n",
    "        test_data_dir: Directory containing test data\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of evaluation results\n",
    "    \"\"\"\n",
    "    # 1. Register test dataset using your existing functions\n",
    "    test_dataset_name = \"cell_test\"\n",
    "    \n",
    "    # First clear the dataset if it's already registered\n",
    "    if test_dataset_name in DatasetCatalog:\n",
    "        DatasetCatalog.remove(test_dataset_name)\n",
    "    \n",
    "    # Register using your function\n",
    "    DatasetCatalog.register(test_dataset_name, lambda d=test_data_dir: get_cell_dicts(d))\n",
    "    MetadataCatalog.get(test_dataset_name).set(thing_classes=[\"cell\"])\n",
    "    \n",
    "    # 2. Configure model for inference\n",
    "    trained_cfg.MODEL.WEIGHTS = os.path.join(trained_cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "    trained_cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.3  # Lower threshold for evaluation\n",
    "    predictor = DefaultPredictor(trained_cfg)\n",
    "    \n",
    "    # 3. Create output directory for evaluation results\n",
    "    eval_output_dir = os.path.join(trained_cfg.OUTPUT_DIR, \"test_evaluation\")\n",
    "    os.makedirs(eval_output_dir, exist_ok=True)\n",
    "    \n",
    "    # 4. Setup evaluator\n",
    "    evaluator = COCOEvaluator(\n",
    "        test_dataset_name, \n",
    "        output_dir=eval_output_dir,\n",
    "        tasks=(\"bbox\", \"segm\"),  # Evaluate both bounding boxes and segmentation\n",
    "        allow_cached_coco=False  # Don't use cached results\n",
    "    )\n",
    "    \n",
    "    # 5. Create data loader for test set\n",
    "    test_loader = build_detection_test_loader(trained_cfg, test_dataset_name)\n",
    "    \n",
    "    # 6. Run evaluation\n",
    "    print(f\"Running evaluation on {test_dataset_name}...\")\n",
    "    eval_results = inference_on_dataset(\n",
    "        predictor.model, \n",
    "        test_loader, \n",
    "        evaluator\n",
    "    )\n",
    "    \n",
    "    # 7. Print detailed results\n",
    "    print(\"\\n=== Evaluation Results ===\")\n",
    "    for task, metrics in eval_results.items():\n",
    "        print(f\"\\n{task} metrics:\")\n",
    "        for metric_name, value in metrics.items():\n",
    "            print(f\"  {metric_name}: {value}\")\n",
    "    \n",
    "    # 8. Generate visualization of results on a few test images\n",
    "    visualize_test_results(trained_cfg, test_dataset_name, eval_output_dir, num_images=5)\n",
    "    \n",
    "    # 9. Save results to file\n",
    "    with open(os.path.join(eval_output_dir, \"metrics.json\"), \"w\") as f:\n",
    "        json.dump(eval_results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nFull evaluation results saved to: {eval_output_dir}\")\n",
    "    \n",
    "    return eval_results\n",
    "\n",
    "def visualize_test_results(cfg, dataset_name, output_dir, num_images=5):\n",
    "    \"\"\"\n",
    "    Visualize model predictions on test images\n",
    "    \n",
    "    Args:\n",
    "        cfg: Trained config\n",
    "        dataset_name: Test dataset name\n",
    "        output_dir: Directory to save visualizations\n",
    "        num_images: Number of images to visualize\n",
    "    \"\"\"\n",
    "    # Setup predictor\n",
    "    predictor = DefaultPredictor(cfg)\n",
    "    \n",
    "    # Get dataset\n",
    "    dataset_dicts = DatasetCatalog.get(dataset_name)\n",
    "    \n",
    "    # Create visualization directory\n",
    "    vis_dir = os.path.join(output_dir, \"visualizations\")\n",
    "    os.makedirs(vis_dir, exist_ok=True)\n",
    "    \n",
    "    # Randomly select images\n",
    "    random.seed(42)  # For reproducibility\n",
    "    selected_images = random.sample(dataset_dicts, min(num_images, len(dataset_dicts)))\n",
    "    \n",
    "    # Process each selected image\n",
    "    for i, d in enumerate(selected_images):\n",
    "        # Read image\n",
    "        img_path = d[\"file_name\"]\n",
    "        img = cv2.imread(img_path)\n",
    "        \n",
    "        # Get predictions\n",
    "        outputs = predictor(img)\n",
    "        instances = outputs[\"instances\"].to(\"cpu\")\n",
    "        \n",
    "        # Visualize ground truth\n",
    "        v_gt = Visualizer(img[:, :, ::-1], scale=1.0)\n",
    "        v_gt = v_gt.draw_dataset_dict(d)\n",
    "        gt_vis = v_gt.get_image()[:, :, ::-1]\n",
    "        \n",
    "        # Visualize predictions\n",
    "        v_pred = Visualizer(img[:, :, ::-1], scale=1.0)\n",
    "        v_pred = v_pred.draw_instance_predictions(instances)\n",
    "        pred_vis = v_pred.get_image()[:, :, ::-1]\n",
    "        \n",
    "        # Create side-by-side comparison\n",
    "        h, w = img.shape[:2]\n",
    "        comparison = np.zeros((h, w*2, 3), dtype=np.uint8)\n",
    "        comparison[:, :w, :] = gt_vis\n",
    "        comparison[:, w:, :] = pred_vis\n",
    "        \n",
    "        # Add labels\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(comparison, \"Ground Truth\", (10, 30), font, 1, (0, 255, 0), 2)\n",
    "        cv2.putText(comparison, \"Prediction\", (w + 10, 30), font, 1, (0, 255, 0), 2)\n",
    "        \n",
    "        # Save visualization\n",
    "        base_name = os.path.basename(img_path)\n",
    "        output_path = os.path.join(vis_dir, f\"compare_{base_name}\")\n",
    "        cv2.imwrite(output_path, comparison)\n",
    "        \n",
    "        print(f\"Processed image {i+1}/{num_images}: {base_name}\")\n",
    "        print(f\"  Found {len(instances)} predictions\")\n",
    "        print(f\"  Ground truth: {len(d['annotations'])} annotations\")\n",
    "    \n",
    "    print(f\"\\nVisualizations saved to: {vis_dir}\")\n",
    "\n",
    "def analyze_result_statistics(eval_results):\n",
    "    \"\"\"\n",
    "    Generate additional statistics and insights from evaluation results\n",
    "    \n",
    "    Args:\n",
    "        eval_results: Dictionary of evaluation results\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Additional Analysis ===\")\n",
    "    \n",
    "    # For segmentation tasks\n",
    "    if \"segm\" in eval_results:\n",
    "        print(\"\\nSegmentation Metrics:\")\n",
    "        segm_metrics = eval_results[\"segm\"]\n",
    "        \n",
    "        # Overall performance\n",
    "        ap = segm_metrics.get(\"AP\", 0)\n",
    "        print(f\"  Mean AP: {ap:.3f}\")\n",
    "        \n",
    "        # Performance by object size\n",
    "        ap_small = segm_metrics.get(\"AP-small\", 0)\n",
    "        ap_medium = segm_metrics.get(\"AP-medium\", 0)\n",
    "        ap_large = segm_metrics.get(\"AP-large\", 0)\n",
    "        print(f\"  AP by size: small={ap_small:.3f}, medium={ap_medium:.3f}, large={ap_large:.3f}\")\n",
    "        \n",
    "        # Performance at different IoU thresholds\n",
    "        ap50 = segm_metrics.get(\"AP50\", 0)\n",
    "        ap75 = segm_metrics.get(\"AP75\", 0)\n",
    "        print(f\"  AP at IoU: AP50={ap50:.3f}, AP75={ap75:.3f}\")\n",
    "        \n",
    "        # Model usage recommendation\n",
    "        if ap < 0.3:\n",
    "            print(\"\\n  ⚠️ Model performance is low. Consider:\")\n",
    "            print(\"    - Training for more iterations\")\n",
    "            print(\"    - Using data augmentation\")\n",
    "            print(\"    - Checking dataset quality\")\n",
    "        elif ap < 0.5:\n",
    "            print(\"\\n  ⚠️ Model performance is moderate. May work for some applications but consider:\")\n",
    "            print(\"    - Fine-tuning hyperparameters\")\n",
    "            print(\"    - Using a larger or more diverse training set\")\n",
    "        else:\n",
    "            print(\"\\n  ✅ Model performance is good for cell segmentation tasks\")\n",
    "    \n",
    "    # For detection tasks\n",
    "    if \"bbox\" in eval_results:\n",
    "        print(\"\\nBounding Box Detection Metrics:\")\n",
    "        bbox_metrics = eval_results[\"bbox\"]\n",
    "        print(f\"  Mean AP: {bbox_metrics.get('AP', 0):.3f}\")\n",
    "        print(f\"  AP50: {bbox_metrics.get('AP50', 0):.3f}\")\n",
    "\n",
    "def calculate_per_image_metrics(cfg, test_dataset_name):\n",
    "    \"\"\"\n",
    "    Calculate metrics on a per-image basis\n",
    "    \n",
    "    Args:\n",
    "        cfg: Detectron2 config\n",
    "        test_dataset_name: Name of test dataset\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with per-image metrics\n",
    "    \"\"\"\n",
    "    # Setup predictor\n",
    "    predictor = DefaultPredictor(cfg)\n",
    "    \n",
    "    # Get dataset\n",
    "    dataset_dicts = DatasetCatalog.get(test_dataset_name)\n",
    "    \n",
    "    # Results dictionary\n",
    "    per_image_results = {}\n",
    "    \n",
    "    # Process each image\n",
    "    for d in dataset_dicts:\n",
    "        img_id = d[\"image_id\"]\n",
    "        \n",
    "        # Read image\n",
    "        img = cv2.imread(d[\"file_name\"])\n",
    "        \n",
    "        # Get predictions\n",
    "        outputs = predictor(img)\n",
    "        instances = outputs[\"instances\"].to(\"cpu\")\n",
    "        \n",
    "        # Calculate metrics for this image\n",
    "        gt_boxes = [anno[\"bbox\"] for anno in d[\"annotations\"]]\n",
    "        pred_boxes = instances.pred_boxes.tensor.numpy() if len(instances) > 0 else []\n",
    "        \n",
    "        # Count predictions and ground truth\n",
    "        num_gt = len(gt_boxes)\n",
    "        num_pred = len(pred_boxes)\n",
    "        \n",
    "        # Simple statistics for this image\n",
    "        per_image_results[img_id] = {\n",
    "            \"num_gt\": num_gt,\n",
    "            \"num_pred\": num_pred,\n",
    "            \"filename\": os.path.basename(d[\"file_name\"])\n",
    "        }\n",
    "    \n",
    "    return per_image_results\n",
    "\n",
    "def main(trained_cfg, test_data_dir):\n",
    "    \"\"\"\n",
    "    Main function to evaluate model on test data\n",
    "    \n",
    "    Args:\n",
    "        trained_cfg: Trained detectron2 config\n",
    "        test_data_dir: Directory containing test data\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of evaluation results\n",
    "    \"\"\"\n",
    "    # Evaluate model on test data\n",
    "    print(f\"Evaluating model on test data from: {test_data_dir}\")\n",
    "    eval_results = evaluate_on_test_data(trained_cfg, test_data_dir)\n",
    "    \n",
    "    # Generate additional analysis\n",
    "    analyze_result_statistics(eval_results)\n",
    "    \n",
    "    # Calculate per-image metrics\n",
    "    per_image_metrics = calculate_per_image_metrics(trained_cfg, \"cell_test\")\n",
    "    \n",
    "    # Save per-image metrics\n",
    "    with open(os.path.join(trained_cfg.OUTPUT_DIR, \"test_evaluation\", \"per_image_metrics.json\"), \"w\") as f:\n",
    "        json.dump(per_image_metrics, f, indent=2)\n",
    "    \n",
    "    # Print summary of per-image metrics\n",
    "    total_gt = sum(img[\"num_gt\"] for img in per_image_metrics.values())\n",
    "    total_pred = sum(img[\"num_pred\"] for img in per_image_metrics.values())\n",
    "    \n",
    "    print(\"\\n=== Per-Image Summary ===\")\n",
    "    print(f\"Total images: {len(per_image_metrics)}\")\n",
    "    print(f\"Total ground truth instances: {total_gt}\")\n",
    "    print(f\"Total predicted instances: {total_pred}\")\n",
    "    print(f\"Average instances per image: {total_gt / len(per_image_metrics):.2f}\")\n",
    "    print(f\"Average predictions per image: {total_pred / len(per_image_metrics):.2f}\")\n",
    "    \n",
    "    return eval_results\n",
    "\n",
    "# Usage:\n",
    "test_results = main(trained_cfg, \"/Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on New Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using trained_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_new_images(cfg, image_paths):\n",
    "    \"\"\"\n",
    "    Run inference on new images\n",
    "    \n",
    "    Args:\n",
    "        cfg: Detectron2 config\n",
    "        image_paths: List of paths to images\n",
    "    \"\"\"\n",
    "    # Load the trained model\n",
    "    cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # Confidence threshold\n",
    "    predictor = DefaultPredictor(cfg)\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = os.path.join(cfg.OUTPUT_DIR, \"new_predictions\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Process each image\n",
    "    import cv2\n",
    "    from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "    \n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        # Read image\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            print(f\"Could not read image: {image_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Run inference\n",
    "        outputs = predictor(img)\n",
    "        instances = outputs[\"instances\"].to(\"cpu\")\n",
    "        \n",
    "        # Visualize results\n",
    "        v = Visualizer(\n",
    "            img[:, :, ::-1],\n",
    "            scale=1.0,\n",
    "            instance_mode=ColorMode.SEGMENTATION\n",
    "        )\n",
    "        v = v.draw_instance_predictions(instances)\n",
    "        result_img = v.get_image()[:, :, ::-1]\n",
    "        \n",
    "        # Save results\n",
    "        output_path = os.path.join(output_dir, f\"pred_{os.path.basename(image_path)}\")\n",
    "        cv2.imwrite(output_path, result_img)\n",
    "        \n",
    "        print(f\"Processed {image_path}: Found {len(instances)} cell instances\")\n",
    "        \n",
    "        # Extract mask data if needed\n",
    "        masks = instances.pred_masks.numpy() if instances.has(\"pred_masks\") else None\n",
    "        scores = instances.scores.numpy() if instances.has(\"scores\") else None\n",
    "        \n",
    "        # You can save masks or do further analysis here\n",
    "    \n",
    "    print(f\"All predictions saved to {output_dir}\")\n",
    "\n",
    "# Example usage\n",
    "# image_paths = ['/Users/niti/Desktop/macOS_D/sem8/CP303/Compilation-Data-170325/FISH-All-Consolidated-Data/Cell-Only/AL 224_23.tif']\n",
    "# predict_on_new_images(trained_cfg, image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[05/12 02:04:44 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from ./cell_segmentation_output/model_final.pth ...\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/AL 276_24_KMT2A.png: Found 19 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MBN 80_24_CCND1_BA_XL.png: Found 6 cell instances\n",
      "All predictions saved to ./cell_segmentation_output/new_predictions\n"
     ]
    }
   ],
   "source": [
    "image_paths = ['/Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/AL 276_24_KMT2A.png', '/Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MBN 80_24_CCND1_BA_XL.png']\n",
    "predict_on_new_images(trained_cfg, image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Saved Model + Save Instance Masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[05/12 09:55:52 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from ./cell_segmentation_output/model_final.pth ...\n"
     ]
    }
   ],
   "source": [
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor\n",
    "import os\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "def load_cell_segmentation_model(model_path):\n",
    "    \"\"\"\n",
    "    Load a trained cell segmentation model\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to model_final.pth file\n",
    "    \n",
    "    Returns:\n",
    "        cfg: Configuration\n",
    "        predictor: Loaded model predictor\n",
    "    \"\"\"\n",
    "    # Get a base config\n",
    "    cfg = get_cfg()\n",
    "    \n",
    "    # Add your model's configuration\n",
    "    # This part needs to match how you configured the training\n",
    "    cfg_path = \"/opt/anaconda3/envs/detectron2-env/lib/python3.9/site-packages/detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"\n",
    "    cfg.merge_from_file(cfg_path)\n",
    "    cfg.MODEL.DEVICE = \"cpu\"\n",
    "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # Set to match your original training\n",
    "    \n",
    "    # Set model path and inference threshold\n",
    "    cfg.MODEL.WEIGHTS = model_path\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7\n",
    "    \n",
    "    # Create predictor\n",
    "    predictor = DefaultPredictor(cfg)\n",
    "    \n",
    "    return cfg, predictor\n",
    "\n",
    "def prediction_maker(predictor, image_paths):\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = os.path.join(cfg.OUTPUT_DIR, \"new_predictions\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    combined_masks_dir = os.path.join(cfg.OUTPUT_DIR, \"instance_masks_combined\")\n",
    "    os.makedirs(combined_masks_dir, exist_ok=True)\n",
    "    masks_dir = os.path.join(cfg.OUTPUT_DIR, \"instance_masks\")\n",
    "    os.makedirs(masks_dir, exist_ok=True)\n",
    "\n",
    "    # Process each image\n",
    "    import cv2\n",
    "    from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "    \n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        # Read image\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            print(f\"Could not read image: {image_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Run inference\n",
    "        outputs = predictor(img)\n",
    "        instances = outputs[\"instances\"].to(\"cpu\")\n",
    "        \n",
    "        # Visualize results\n",
    "        v = Visualizer(\n",
    "            img[:, :, ::-1],\n",
    "            scale=1.0,\n",
    "            instance_mode=ColorMode.SEGMENTATION\n",
    "        )\n",
    "        v = v.draw_instance_predictions(instances)\n",
    "        result_img = v.get_image()[:, :, ::-1]\n",
    "        \n",
    "        # Save results\n",
    "        output_path = os.path.join(output_dir, f\"pred_{os.path.basename(image_path)}\")\n",
    "        cv2.imwrite(output_path, result_img)\n",
    "        \n",
    "        print(f\"Processed {image_path}: Found {len(instances)} cell instances\")\n",
    "        \n",
    "        # Extract mask data if needed\n",
    "        masks = instances.pred_masks.numpy() if instances.has(\"pred_masks\") else None\n",
    "        scores = instances.scores.numpy() if instances.has(\"scores\") else None\n",
    "\n",
    "        if masks is not None:\n",
    "            base_filename = os.path.splitext(os.path.basename(image_path))[0]\n",
    "            num_instances = masks.shape[0]\n",
    "\n",
    "            # Combined Mask Image\n",
    "            if num_instances > 0:\n",
    "                # Create a color-coded mask image where each instance has a different color\n",
    "                height, width = masks[0].shape\n",
    "                combined_mask = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "                \n",
    "                # Generate unique colors for each instance\n",
    "                colors = [\n",
    "                    (np.random.randint(0, 255), np.random.randint(0, 255), np.random.randint(0, 255))\n",
    "                    for _ in range(num_instances)\n",
    "                ]\n",
    "                \n",
    "                # Add each mask with its unique color\n",
    "                for j, mask in enumerate(masks):\n",
    "                    color_mask = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "                    color_mask[mask] = colors[j]\n",
    "                    combined_mask = cv.bitwise_or(combined_mask, color_mask)\n",
    "                \n",
    "                # Save combined mask\n",
    "                combined_mask_path = os.path.join(combined_masks_dir, f\"{base_filename}_all_instances.png\")\n",
    "                cv2.imwrite(combined_mask_path, combined_mask)\n",
    "            \n",
    "            # Single Masks Saved\n",
    "            if num_instances > 0:\n",
    "                current_masks_dir = os.path.join(masks_dir, base_filename)\n",
    "                os.makedirs(current_masks_dir, exist_ok=True)\n",
    "\n",
    "                for j, mask in enumerate(masks):\n",
    "                    # Create a blank grayscale image\n",
    "                    single_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "                    \n",
    "                    # Set mask area to white (255)\n",
    "                    single_mask[mask] = 255\n",
    "                    \n",
    "                    # Save the single instance mask\n",
    "                    single_mask_path = os.path.join(current_masks_dir, f\"{base_filename}_instance_{j+1}.png\")\n",
    "                    cv2.imwrite(single_mask_path, single_mask)\n",
    "                    \n",
    "        # You can save masks or do further analysis here\n",
    "    \n",
    "    print(f\"All predictions saved to {output_dir}\")\n",
    "# Usage\n",
    "model_path = \"./cell_segmentation_output/model_final.pth\"\n",
    "cfg, predictor = load_cell_segmentation_model(model_path)\n",
    "# image_paths = ['/Users/niti/Desktop/macOS_D/sem8/CP303/Compilation-Data-170325/FISH-All-Consolidated-Data/Cell-Only/AL 224_23.tif']\n",
    "# prediction_maker(predictor, image_paths)\n",
    "# Now you can use predictor for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MBN 108_24_CEN11ATM_XL.png: Found 14 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MM 195_24_5P9Q15Q_26_06_2024.png: Found 2 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MBN 80_24_CCND1_BA_XL.png: Found 6 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MDS 601_23_8.png: Found 66 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MM 127_24_PLOIDY_06_05_2024.png: Found 4 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MDS 254_24_CEN8.png: Found 2 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MM 92_24_1Q1P_XL.png: Found 36 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MM 159_24_TP53_NF1.png: Found 5 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/AL 152_24_TCF3_PBX1_XL_1.png: Found 25 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MDS 601_23_5Q_05_01_2024.png: Found 46 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MM 171_24_FGFR3_IGH.png: Found 25 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MDS 581_23_CEN8.png: Found 15 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MDS 106_24_MECOM3_XL.png: Found 12 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/AL 200_24_PLOIDY_XL_02_05_2024.png: Found 3 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MM 142_24_MYC_BA_XL.png: Found 6 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/AL 523_22_CCND1_BA.png: Found 38 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/AL 291_24_PDGFRB.png: Found 1 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MM 125_24_CCND1_BA_XL.png: Found 5 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/Al 180_24_RARA_BA_2.png: Found 6 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MM 94_24_FGFR3_XL_1.png: Found 15 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/AL 123_24_CEN8.png: Found 10 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MDS 589_23_7Q_05_01_2024.png: Found 40 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MM 83_24_1Q1P_XL.png: Found 20 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MDS 1_24_8_18_01_2024.png: Found 77 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MDS 152_24_CEN_7_XL_02_05_2024.png: Found 12 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MM 102_24_MAFIGH_XL.png: Found 3 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/FH 133_24_MYC.png: Found 4 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MM 50_24_1Q1P_XL_1.png: Found 3 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/AL 111_24_BCR_ABL_1_XL_2.png: Found 2 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/AL 276_24_KMT2A.png: Found 19 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MBN 92_24_ATMCEN11_XL.png: Found 22 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MDS 160_24_DEL_20_XL_1.png: Found 8 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MM 130_24_1P_1Q_XL.png: Found 13 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MDS 128_24_BCRABL1_XL.png: Found 20 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MM 92_24_FGFR3_XL.png: Found 16 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MM 170_24_1P1Q.png: Found 8 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/AL 102_24_7Q_CEN17_XL_02_05_2024.png: Found 11 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MM 97_24_1P1Q_XL.png: Found 14 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/AL 157_24_TCF3_PBX1_XL.png: Found 6 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MM 158_24_MYC.png: Found 2 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MM 209_24_CCND1.png: Found 4 cell instances\n",
      "Processed /Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images/MBN 155_24_TP53_CEN17.png: Found 2 cell instances\n",
      "All predictions saved to ./output/new_predictions\n"
     ]
    }
   ],
   "source": [
    "image_paths = []\n",
    "test_images_dir = \"/Users/vipulpatil/Desktop/FISH-Instance-Net/Test-Dataset/images\"\n",
    "for filename in os.listdir(test_images_dir):\n",
    "    if filename.endswith(('.png', '.jpg', '.tif')):\n",
    "        image_paths.append(os.path.join(test_images_dir, filename))\n",
    "prediction_maker(predictor, image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectron2-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
